{
	"schemaVersion": 1,
	"source": "github",
	"owner": "openclaw",
	"repo": "openclaw",
	"sha": "5a475259bb4af18da06aab0eb8ceb86b50caf5f2",
	"htmlUrl": "https://github.com/openclaw/openclaw/commit/5a475259bb4af18da06aab0eb8ceb86b50caf5f2",
	"apiUrl": "https://api.github.com/repos/openclaw/openclaw/commits/5a475259bb4af18da06aab0eb8ceb86b50caf5f2",
	"diffUrl": "https://github.com/openclaw/openclaw/commit/5a475259bb4af18da06aab0eb8ceb86b50caf5f2.diff",
	"message": "fix(telegram): suppress reasoning-only leaks when reasoning is off\n\nCo-authored-by: avirweb <avirweb@users.noreply.github.com>",
	"authorName": "Peter Steinberger",
	"authorEmail": "steipete@gmail.com",
	"authoredAt": "2026-02-23T20:05:45Z",
	"committerName": "Peter Steinberger",
	"committerEmail": "steipete@gmail.com",
	"committedAt": "2026-02-23T20:06:16Z",
	"parents": ["63e4dfaa9c95f73783677a96df6ba49b13f24caf"],
	"stats": {
		"additions": 73,
		"deletions": 21,
		"total": 94
	},
	"files": [
		{
			"filename": "CHANGELOG.md",
			"status": "modified",
			"additions": 1,
			"deletions": 0,
			"changes": 1,
			"patch": "@@ -24,6 +24,7 @@ Docs: https://docs.openclaw.ai\n - Sessions/Store: canonicalize inbound mixed-case session keys for metadata and route updates, and migrate legacy case-variant entries to a single lowercase key to prevent duplicate sessions and missing TUI/WebUI history. (#9561) Thanks @hillghost86.\n - Telegram/Reactions: soft-fail reaction action errors (policy/token/emoji/API), accept snake_case `message_id`, and fallback to inbound message-id context when explicit `messageId` is omitted so DM reactions stay stable without regeneration loops. (#20236, #21001) Thanks @PeterShanxin and @vincentkoc.\n - Telegram/Polling: scope persisted polling offsets to bot identity and reuse a single awaited runner-stop path on abort/retry, preventing cross-token offset bleed and overlapping pollers during restart/error recovery. (#10850, #11347) Thanks @talhaorak, @anooprdawar, and @vincentkoc.\n+- Telegram/Reasoning: when `/reasoning off` is active, suppress reasoning-only delivery segments and block raw fallback resend of suppressed `Reasoning:`/`<think>` text, preventing internal reasoning leakage in legacy sessions while preserving answer delivery. (#24626, #24518)\n - Agents/Reasoning: when model-default thinking is active (for example `thinking=low`), keep auto-reasoning disabled unless explicitly enabled, preventing `Reasoning:` thinking-block leakage in channel replies. (#24335, #24290) thanks @Kay-051.\n - Agents/Reasoning: avoid classifying provider reasoning-required errors as context overflows so these failures no longer trigger compaction-style overflow recovery. (#24593) Thanks @vincentkoc.\n - Agents/Models: codify `agents.defaults.model` / `agents.defaults.imageModel` config-boundary input as `string | {primary,fallbacks}`, split explicit vs effective model resolution, and fix `models status --agent` source attribution so defaults-inherited agents are labeled as `defaults` while runtime selection still honors defaults fallback. (#24210) thanks @bianbiandashen."
		},
		{
			"filename": "src/telegram/bot-message-dispatch.test.ts",
			"status": "modified",
			"additions": 45,
			"deletions": 16,
			"changes": 61,
			"patch": "@@ -176,6 +176,15 @@ describe(\"dispatchTelegramMessage draft streaming\", () => {\n     });\n   }\n \n+  function createReasoningStreamContext(): TelegramMessageContext {\n+    loadSessionStore.mockReturnValue({\n+      s1: { reasoningLevel: \"stream\" },\n+    });\n+    return createContext({\n+      ctxPayload: { SessionKey: \"s1\" } as unknown as TelegramMessageContext[\"ctxPayload\"],\n+    });\n+  }\n+\n   it(\"streams drafts in private threads and forwards thread id\", async () => {\n     const draftStream = createDraftStream();\n     createTelegramDraftStream.mockReturnValue(draftStream);\n@@ -772,7 +781,7 @@ describe(\"dispatchTelegramMessage draft streaming\", () => {\n       deliverReplies.mockResolvedValue({ delivered: true });\n       editMessageTelegram.mockResolvedValue({ ok: true, chatId: \"123\", messageId: \"999\" });\n \n-      await dispatchWithContext({ context: createContext(), streamMode });\n+      await dispatchWithContext({ context: createReasoningStreamContext(), streamMode });\n \n       expect(reasoningDraftStream.forceNewMessage).toHaveBeenCalledTimes(1);\n     },\n@@ -809,7 +818,11 @@ describe(\"dispatchTelegramMessage draft streaming\", () => {\n     editMessageTelegram.mockResolvedValue({ ok: true, chatId: \"123\", messageId: \"999\" });\n \n     const bot = createBot();\n-    await dispatchWithContext({ context: createContext(), streamMode: \"partial\", bot });\n+    await dispatchWithContext({\n+      context: createReasoningStreamContext(),\n+      streamMode: \"partial\",\n+      bot,\n+    });\n \n     expect(reasoningDraftParams?.onSupersededPreview).toBeTypeOf(\"function\");\n     const deleteMessageCalls = (\n@@ -836,13 +849,13 @@ describe(\"dispatchTelegramMessage draft streaming\", () => {\n       );\n       deliverReplies.mockResolvedValue({ delivered: true });\n \n-      await dispatchWithContext({ context: createContext(), streamMode });\n+      await dispatchWithContext({ context: createReasoningStreamContext(), streamMode });\n \n       expect(reasoningDraftStream.forceNewMessage).not.toHaveBeenCalled();\n     },\n   );\n \n-  it(\"does not finalize preview with reasoning payloads before answer payloads\", async () => {\n+  it(\"suppresses reasoning-only final payloads when reasoning level is off\", async () => {\n     setupDraftStreams({ answerMessageId: 999 });\n     dispatchReplyWithBufferedBlockDispatcher.mockImplementation(\n       async ({ dispatcherOptions, replyOptions }) => {\n@@ -860,14 +873,11 @@ describe(\"dispatchTelegramMessage draft streaming\", () => {\n \n     await dispatchWithContext({ context: createContext(), streamMode: \"partial\" });\n \n-    // Keep reasoning as its own message.\n-    expect(deliverReplies).toHaveBeenCalledTimes(1);\n-    expect(deliverReplies).toHaveBeenCalledWith(\n+    expect(deliverReplies).not.toHaveBeenCalledWith(\n       expect.objectContaining({\n         replies: [expect.objectContaining({ text: \"Reasoning:\\n_step one_\" })],\n       }),\n     );\n-    // Finalize preview with the actual answer instead of overwriting with reasoning.\n     expect(editMessageTelegram).toHaveBeenCalledTimes(1);\n     expect(editMessageTelegram).toHaveBeenCalledWith(\n       123,\n@@ -877,6 +887,25 @@ describe(\"dispatchTelegramMessage draft streaming\", () => {\n     );\n   });\n \n+  it(\"does not resend suppressed reasoning-only text through raw fallback\", async () => {\n+    setupDraftStreams({ answerMessageId: 999 });\n+    dispatchReplyWithBufferedBlockDispatcher.mockImplementation(async ({ dispatcherOptions }) => {\n+      await dispatcherOptions.deliver({ text: \"Reasoning:\\n_step one_\" }, { kind: \"final\" });\n+      return { queuedFinal: true };\n+    });\n+    deliverReplies.mockResolvedValue({ delivered: true });\n+    editMessageTelegram.mockResolvedValue({ ok: true, chatId: \"123\", messageId: \"999\" });\n+\n+    await dispatchWithContext({ context: createContext(), streamMode: \"partial\" });\n+\n+    expect(deliverReplies).not.toHaveBeenCalledWith(\n+      expect.objectContaining({\n+        replies: [expect.objectContaining({ text: \"Reasoning:\\n_step one_\" })],\n+      }),\n+    );\n+    expect(editMessageTelegram).not.toHaveBeenCalled();\n+  });\n+\n   it(\"keeps reasoning and answer streaming in separate preview lanes\", async () => {\n     const { answerDraftStream, reasoningDraftStream } = setupDraftStreams({\n       answerMessageId: 999,\n@@ -893,7 +922,7 @@ describe(\"dispatchTelegramMessage draft streaming\", () => {\n     deliverReplies.mockResolvedValue({ delivered: true });\n     editMessageTelegram.mockResolvedValue({ ok: true, chatId: \"123\", messageId: \"999\" });\n \n-    await dispatchWithContext({ context: createContext(), streamMode: \"partial\" });\n+    await dispatchWithContext({ context: createReasoningStreamContext(), streamMode: \"partial\" });\n \n     expect(reasoningDraftStream.update).toHaveBeenCalledWith(\"Reasoning:\\n_Working on it..._\");\n     expect(answerDraftStream.update).toHaveBeenCalledWith(\"Checking the directory...\");\n@@ -913,7 +942,7 @@ describe(\"dispatchTelegramMessage draft streaming\", () => {\n     deliverReplies.mockResolvedValue({ delivered: true });\n     editMessageTelegram.mockResolvedValue({ ok: true, chatId: \"123\", messageId: \"999\" });\n \n-    await dispatchWithContext({ context: createContext(), streamMode: \"partial\" });\n+    await dispatchWithContext({ context: createReasoningStreamContext(), streamMode: \"partial\" });\n \n     expect(editMessageTelegram).not.toHaveBeenCalled();\n     expect(deliverReplies).toHaveBeenCalledWith(\n@@ -955,7 +984,7 @@ describe(\"dispatchTelegramMessage draft streaming\", () => {\n       deliverReplies.mockResolvedValue({ delivered: true });\n       editMessageTelegram.mockResolvedValue({ ok: true, chatId: \"123\", messageId: \"111\" });\n \n-      await dispatchWithContext({ context: createContext(), streamMode });\n+      await dispatchWithContext({ context: createReasoningStreamContext(), streamMode });\n \n       expect(reasoningDraftStream.forceNewMessage).not.toHaveBeenCalled();\n       expect(editMessageTelegram).toHaveBeenCalledWith(\n@@ -990,7 +1019,7 @@ describe(\"dispatchTelegramMessage draft streaming\", () => {\n     deliverReplies.mockResolvedValue({ delivered: true });\n     editMessageTelegram.mockResolvedValue({ ok: true, chatId: \"123\", messageId: \"999\" });\n \n-    await dispatchWithContext({ context: createContext(), streamMode: \"partial\" });\n+    await dispatchWithContext({ context: createReasoningStreamContext(), streamMode: \"partial\" });\n \n     expect(editMessageTelegram).toHaveBeenNthCalledWith(1, 123, 999, \"3\", expect.any(Object));\n     expect(editMessageTelegram).toHaveBeenNthCalledWith(\n@@ -1028,7 +1057,7 @@ describe(\"dispatchTelegramMessage draft streaming\", () => {\n     deliverReplies.mockResolvedValue({ delivered: true });\n     editMessageTelegram.mockResolvedValue({ ok: true, chatId: \"123\", messageId: \"999\" });\n \n-    await dispatchWithContext({ context: createContext(), streamMode: \"partial\" });\n+    await dispatchWithContext({ context: createReasoningStreamContext(), streamMode: \"partial\" });\n \n     expect(reasoningDraftStream.update).toHaveBeenCalledWith(\n       \"Reasoning:\\n_Counting letters in strawberry_\",\n@@ -1060,7 +1089,7 @@ describe(\"dispatchTelegramMessage draft streaming\", () => {\n     deliverReplies.mockResolvedValue({ delivered: true });\n     editMessageTelegram.mockResolvedValue({ ok: true, chatId: \"123\", messageId: \"999\" });\n \n-    await dispatchWithContext({ context: createContext(), streamMode: \"partial\" });\n+    await dispatchWithContext({ context: createReasoningStreamContext(), streamMode: \"partial\" });\n \n     expect(reasoningDraftStream.update).toHaveBeenCalledWith(\n       \"Reasoning:\\n_Counting letters in strawberry_\",\n@@ -1096,7 +1125,7 @@ describe(\"dispatchTelegramMessage draft streaming\", () => {\n     deliverReplies.mockResolvedValue({ delivered: true });\n     editMessageTelegram.mockResolvedValue({ ok: true, chatId: \"123\", messageId: \"999\" });\n \n-    await dispatchWithContext({ context: createContext(), streamMode: \"partial\" });\n+    await dispatchWithContext({ context: createReasoningStreamContext(), streamMode: \"partial\" });\n \n     expect(reasoningDraftStream.update).toHaveBeenCalledWith(\n       \"Reasoning:\\n_Word: strawberry. r appears at 3, 8, 9._\",\n@@ -1127,7 +1156,7 @@ describe(\"dispatchTelegramMessage draft streaming\", () => {\n     deliverReplies.mockResolvedValue({ delivered: true });\n     editMessageTelegram.mockResolvedValue({ ok: true, chatId: \"123\", messageId: \"999\" });\n \n-    await dispatchWithContext({ context: createContext(), streamMode: \"partial\" });\n+    await dispatchWithContext({ context: createReasoningStreamContext(), streamMode: \"partial\" });\n \n     expect(editMessageTelegram).toHaveBeenNthCalledWith(\n       1,"
		},
		{
			"filename": "src/telegram/bot-message-dispatch.ts",
			"status": "modified",
			"additions": 27,
			"deletions": 5,
			"changes": 32,
			"patch": "@@ -202,16 +202,25 @@ export const dispatchTelegramMessage = async ({\n   let splitReasoningOnNextStream = false;\n   const reasoningStepState = createTelegramReasoningStepState();\n   type SplitLaneSegment = { lane: LaneName; text: string };\n-  const splitTextIntoLaneSegments = (text?: string): SplitLaneSegment[] => {\n+  type SplitLaneSegmentsResult = {\n+    segments: SplitLaneSegment[];\n+    suppressedReasoningOnly: boolean;\n+  };\n+  const splitTextIntoLaneSegments = (text?: string): SplitLaneSegmentsResult => {\n     const split = splitTelegramReasoningText(text);\n     const segments: SplitLaneSegment[] = [];\n-    if (split.reasoningText) {\n+    const suppressReasoning = resolvedReasoningLevel === \"off\";\n+    if (split.reasoningText && !suppressReasoning) {\n       segments.push({ lane: \"reasoning\", text: split.reasoningText });\n     }\n     if (split.answerText) {\n       segments.push({ lane: \"answer\", text: split.answerText });\n     }\n-    return segments;\n+    return {\n+      segments,\n+      suppressedReasoningOnly:\n+        Boolean(split.reasoningText) && suppressReasoning && !split.answerText,\n+    };\n   };\n   const resetDraftLaneState = (lane: DraftLaneState) => {\n     lane.lastPartialText = \"\";\n@@ -241,7 +250,8 @@ export const dispatchTelegramMessage = async ({\n     laneStream.update(text);\n   };\n   const ingestDraftLaneSegments = (text: string | undefined) => {\n-    for (const segment of splitTextIntoLaneSegments(text)) {\n+    const split = splitTextIntoLaneSegments(text);\n+    for (const segment of split.segments) {\n       if (segment.lane === \"reasoning\") {\n         reasoningStepState.noteReasoningHint();\n         reasoningStepState.noteReasoningDelivered();\n@@ -418,7 +428,8 @@ export const dispatchTelegramMessage = async ({\n           const previewButtons = (\n             payload.channelData?.telegram as { buttons?: TelegramInlineButtons } | undefined\n           )?.buttons;\n-          const segments = splitTextIntoLaneSegments(payload.text);\n+          const split = splitTextIntoLaneSegments(payload.text);\n+          const segments = split.segments;\n           const hasMedia = Boolean(payload.mediaUrl) || (payload.mediaUrls?.length ?? 0) > 0;\n \n           const flushBufferedFinalAnswer = async () => {\n@@ -478,6 +489,17 @@ export const dispatchTelegramMessage = async ({\n           if (segments.length > 0) {\n             return;\n           }\n+          if (split.suppressedReasoningOnly) {\n+            if (hasMedia) {\n+              const payloadWithoutSuppressedReasoning =\n+                typeof payload.text === \"string\" ? { ...payload, text: \"\" } : payload;\n+              await sendPayload(payloadWithoutSuppressedReasoning);\n+            }\n+            if (info.kind === \"final\") {\n+              await flushBufferedFinalAnswer();\n+            }\n+            return;\n+          }\n \n           if (info.kind === \"final\") {\n             await answerLane.stream?.stop();"
		}
	],
	"fetchedAt": "2026-02-23T23:03:18.070Z"
}
