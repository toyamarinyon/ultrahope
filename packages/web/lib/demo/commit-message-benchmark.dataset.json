{
	"generatedAt": "2026-02-22T22:58:02.340Z",
	"provider": "vercel-ai-gateway",
	"models": [
		{
			"id": "mistral/ministral-3b",
			"tier": "small",
			"label": "Ministral 3B"
		},
		{
			"id": "xai/grok-code-fast-1",
			"tier": "small",
			"label": "Grok Code Fast"
		},
		{
			"id": "cerebras/llama3.1-8b",
			"tier": "small",
			"label": "Llama 3.1 8B"
		},
		{
			"id": "openai/gpt-5.2",
			"tier": "frontier",
			"label": "GPT-5.2"
		},
		{
			"id": "anthropic/claude-opus-4.5",
			"tier": "frontier",
			"label": "Claude Opus 4.5"
		},
		{
			"id": "google/gemini-3-pro-preview",
			"tier": "frontier",
			"label": "Gemini 3 Pro"
		}
	],
	"scenarios": [
		{
			"id": "react-0e72d281-flight-add-more-dos-mitigations-t",
			"title": "[Flight] Add more DoS mitigations to Flight Reply, and harden Flight",
			"sourceRepo": "facebook/react",
			"sourceCommitUrl": "https://github.com/facebook/react/commit/0e72d2810a1c58cb816f5138829dbedaa545d6e2",
			"diff": "diff --git a/packages/react-client/src/ReactFlightClient.js b/packages/react-client/src/ReactFlightClient.js\nindex 0db63ecd3efa..2246f74697eb 100644\n--- a/packages/react-client/src/ReactFlightClient.js\n+++ b/packages/react-client/src/ReactFlightClient.js\n@@ -94,6 +94,8 @@ import getComponentNameFromType from 'shared/getComponentNameFromType';\n \n import {getOwnerStackByComponentInfoInDev} from 'shared/ReactComponentInfoStack';\n \n+import hasOwnProperty from 'shared/hasOwnProperty';\n+\n import {injectInternals} from './ReactFlightClientDevToolsHook';\n \n import {OMITTED_PROP_ERROR} from 'shared/ReactFlightPropertyAccess';\n@@ -159,6 +161,8 @@ const INITIALIZED = 'fulfilled';\n const ERRORED = 'rejected';\n const HALTED = 'halted'; // DEV-only. Means it never resolves even if connection closes.\n \n+const __PROTO__ = '__proto__';\n+\n type PendingChunk<T> = {\n   status: 'pending',\n   value: null | Array<InitializationReference | (T => mixed)>,\n@@ -1544,7 +1548,16 @@ function fulfillReference(\n           }\n         }\n       }\n-      value = value[path[i]];\n+      const name = path[i];\n+      if (\n+        typeof value === 'object' &&\n+        value !== null &&\n+        hasOwnProperty.call(value, name)\n+      ) {\n+        value = value[name];\n+      } else {\n+        throw new Error('Invalid reference.');\n+      }\n     }\n \n     while (\n@@ -1580,7 +1593,9 @@ function fulfillReference(\n     }\n \n     const mappedValue = map(response, value, parentObject, key);\n-    parentObject[key] = mappedValue;\n+    if (key !== __PROTO__) {\n+      parentObject[key] = mappedValue;\n+    }\n \n     // If this is the root object for a model reference, where `handler.value`\n     // is a stale `null`, the resolved value can be used directly.\n@@ -1849,7 +1864,9 @@ function loadServerReference<A: Iterable<any>, T>(\n       response._encodeFormAction,\n     );\n \n-    parentObject[key] = resolvedValue;\n+    if (key !== __PROTO__) {\n+      parentObject[key] = resolvedValue;\n+    }\n \n     // If this is the root object for a model reference, where `handler.value`\n     // is a stale `null`, the resolved value can be used directly.\n@@ -2231,29 +2248,31 @@ function defineLazyGetter<T>(\n ): any {\n   // We don't immediately initialize it even if it's resolved.\n   // Instead, we wait for the getter to get accessed.\n-  Object.defineProperty(parentObject, key, {\n-    get: function () {\n-      if (chunk.status === RESOLVED_MODEL) {\n-        // If it was now resolved, then we initialize it. This may then discover\n-        // a new set of lazy references that are then asked for eagerly in case\n-        // we get that deep.\n-        initializeModelChunk(chunk);\n-      }\n-      switch (chunk.status) {\n-        case INITIALIZED: {\n-          return chunk.value;\n+  if (key !== __PROTO__) {\n+    Object.defineProperty(parentObject, key, {\n+      get: function () {\n+        if (chunk.status === RESOLVED_MODEL) {\n+          // If it was now resolved, then we initialize it. This may then discover\n+          // a new set of lazy references that are then asked for eagerly in case\n+          // we get that deep.\n+          initializeModelChunk(chunk);\n         }\n-        case ERRORED:\n-          throw chunk.reason;\n-      }\n-      // Otherwise, we didn't have enough time to load the object before it was\n-      // accessed or the connection closed. So we just log that it was omitted.\n-      // TODO: We should ideally throw here to indicate a difference.\n-      return OMITTED_PROP_ERROR;\n-    },\n-    enumerable: true,\n-    configurable: false,\n-  });\n+        switch (chunk.status) {\n+          case INITIALIZED: {\n+            return chunk.value;\n+          }\n+          case ERRORED:\n+            throw chunk.reason;\n+        }\n+        // Otherwise, we didn't have enough time to load the object before it was\n+        // accessed or the connection closed. So we just log that it was omitted.\n+        // TODO: We should ideally throw here to indicate a difference.\n+        return OMITTED_PROP_ERROR;\n+      },\n+      enumerable: true,\n+      configurable: false,\n+    });\n+  }\n   return null;\n }\n \n@@ -2564,14 +2583,16 @@ function parseModelString(\n           // In DEV mode we encode omitted objects in logs as a getter that throws\n           // so that when you try to access it on the client, you know why that\n           // happened.\n-          Object.defineProperty(parentObject, key, {\n-            get: function () {\n-              // TODO: We should ideally throw here to indicate a difference.\n-              return OMITTED_PROP_ERROR;\n-            },\n-            enumerable: true,\n-            configurable: false,\n-          });\n+          if (key !== __PROTO__) {\n+            Object.defineProperty(parentObject, key, {\n+              get: function () {\n+                // TODO: We should ideally throw here to indicate a difference.\n+                return OMITTED_PROP_ERROR;\n+              },\n+              enumerable: true,\n+              configurable: false,\n+            });\n+          }\n           return null;\n         }\n         // Fallthrough\n@@ -5183,6 +5204,9 @@ function parseModel<T>(response: Response, json: UninitializedModel): T {\n function createFromJSONCallback(response: Response) {\n   // $FlowFixMe[missing-this-annot]\n   return function (key: string, value: JSONValue) {\n+    if (key === __PROTO__) {\n+      return undefined;\n+    }\n     if (typeof value === 'string') {\n       // We can't use .bind here because we need the \"this\" value.\n       return parseModelString(response, this, key, value);\ndiff --git a/packages/react-client/src/ReactFlightReplyClient.js b/packages/react-client/src/ReactFlightReplyClient.js\nindex 4dc13ce48607..9a1b6651868c 100644\n--- a/packages/react-client/src/ReactFlightReplyClient.js\n+++ b/packages/react-client/src/ReactFlightReplyClient.js\n@@ -95,6 +95,8 @@ export type ReactServerValue =\n \n type ReactServerObject = {+[key: string]: ReactServerValue};\n \n+const __PROTO__ = '__proto__';\n+\n function serializeByValueID(id: number): string {\n   return '$' + id.toString(16);\n }\n@@ -361,6 +363,15 @@ export function processReply(\n   ): ReactJSONValue {\n     const parent = this;\n \n+    if (__DEV__) {\n+      if (key === __PROTO__) {\n+        console.error(\n+          'Expected not to serialize an object with own property `__proto__`. When parsed this property will be omitted.%s',\n+          describeObjectForErrorMessage(parent, key),\n+        );\n+      }\n+    }\n+\n     // Make sure that `parent[key]` wasn't JSONified before `value` was passed to us\n     if (__DEV__) {\n       // $FlowFixMe[incompatible-use]\n@@ -780,6 +791,10 @@ export function processReply(\n     if (typeof value === 'function') {\n       const referenceClosure = knownServerReferences.get(value);\n       if (referenceClosure !== undefined) {\n+        const existingReference = writtenObjects.get(value);\n+        if (existingReference !== undefined) {\n+          return existingReference;\n+        }\n         const {id, bound} = referenceClosure;\n         const referenceClosureJSON = JSON.stringify({id, bound}, resolveToJSON);\n         if (formData === null) {\n@@ -789,7 +804,10 @@ export function processReply(\n         // The reference to this function came from the same client so we can pass it back.\n         const refId = nextPartId++;\n         formData.set(formFieldPrefix + refId, referenceClosureJSON);\n-        return serializeServerReferenceID(refId);\n+        const serverReferenceId = serializeServerReferenceID(refId);\n+        // Store the server reference ID for deduplication.\n+        writtenObjects.set(value, serverReferenceId);\n+        return serverReferenceId;\n       }\n       if (temporaryReferences !== undefined && key.indexOf(':') === -1) {\n         // TODO: If the property name contains a colon, we don't dedupe. Escape instead.\ndiff --git a/packages/react-server-dom-esm/src/server/ReactFlightDOMServerNode.js b/packages/react-server-dom-esm/src/server/ReactFlightDOMServerNode.js\nindex b81e177d642e..8e0799fb0209 100644\n--- a/packages/react-server-dom-esm/src/server/ReactFlightDOMServerNode.js\n+++ b/packages/react-server-dom-esm/src/server/ReactFlightDOMServerNode.js\n@@ -328,12 +328,17 @@ function prerenderToNodeStream(\n function decodeReplyFromBusboy<T>(\n   busboyStream: Busboy,\n   moduleBasePath: ServerManifest,\n-  options?: {temporaryReferences?: TemporaryReferenceSet},\n+  options?: {\n+    temporaryReferences?: TemporaryReferenceSet,\n+    arraySizeLimit?: number,\n+  },\n ): Thenable<T> {\n   const response = createResponse(\n     moduleBasePath,\n     '',\n     options ? options.temporaryReferences : undefined,\n+    undefined,\n+    options ? options.arraySizeLimit : undefined,\n   );\n   let pendingFiles = 0;\n   const queuedFields: Array<string> = [];\n@@ -399,7 +404,10 @@ function decodeReplyFromBusboy<T>(\n function decodeReply<T>(\n   body: string | FormData,\n   moduleBasePath: ServerManifest,\n-  options?: {temporaryReferences?: TemporaryReferenceSet},\n+  options?: {\n+    temporaryReferences?: TemporaryReferenceSet,\n+    arraySizeLimit?: number,\n+  },\n ): Thenable<T> {\n   if (typeof body === 'string') {\n     const form = new FormData();\n@@ -411,6 +419,7 @@ function decodeReply<T>(\n     '',\n     options ? options.temporaryReferences : undefined,\n     body,\n+    options ? options.arraySizeLimit : undefined,\n   );\n   const root = getRoot<T>(response);\n   close(response);\ndiff --git a/packages/react-server-dom-parcel/src/server/ReactFlightDOMServerBrowser.js b/packages/react-server-dom-parcel/src/server/ReactFlightDOMServerBrowser.js\nindex 31c3e5cd8b66..bdaadd66684a 100644\n--- a/packages/react-server-dom-parcel/src/server/ReactFlightDOMServerBrowser.js\n+++ b/packages/react-server-dom-parcel/src/server/ReactFlightDOMServerBrowser.js\n@@ -245,7 +245,10 @@ export function registerServerActions(manifest: ServerManifest) {\n \n export function decodeReply<T>(\n   body: string | FormData,\n-  options?: {temporaryReferences?: TemporaryReferenceSet},\n+  options?: {\n+    temporaryReferences?: TemporaryReferenceSet,\n+    arraySizeLimit?: number,\n+  },\n ): Thenable<T> {\n   if (typeof body === 'string') {\n     const form = new FormData();\n@@ -257,6 +260,7 @@ export function decodeReply<T>(\n     '',\n     options ? options.temporaryReferences : undefined,\n     body,\n+    options ? options.arraySizeLimit : undefined,\n   );\n   const root = getRoot<T>(response);\n   close(response);\ndiff --git a/packages/react-server-dom-parcel/src/server/ReactFlightDOMServerEdge.js b/packages/react-server-dom-parcel/src/server/ReactFlightDOMServerEdge.js\nindex 63218160d8da..83150996ae69 100644\n--- a/packages/react-server-dom-parcel/src/server/ReactFlightDOMServerEdge.js\n+++ b/packages/react-server-dom-parcel/src/server/ReactFlightDOMServerEdge.js\n@@ -250,7 +250,10 @@ export function registerServerActions(manifest: ServerManifest) {\n \n export function decodeReply<T>(\n   body: string | FormData,\n-  options?: {temporaryReferences?: TemporaryReferenceSet},\n+  options?: {\n+    temporaryReferences?: TemporaryReferenceSet,\n+    arraySizeLimit?: number,\n+  },\n ): Thenable<T> {\n   if (typeof body === 'string') {\n     const form = new FormData();\n@@ -262,6 +265,7 @@ export function decodeReply<T>(\n     '',\n     options ? options.temporaryReferences : undefined,\n     body,\n+    options ? options.arraySizeLimit : undefined,\n   );\n   const root = getRoot<T>(response);\n   close(response);\ndiff --git a/packages/react-server-dom-parcel/src/server/ReactFlightDOMServerNode.js b/packages/react-server-dom-parcel/src/server/ReactFlightDOMServerNode.js\nindex 294e99e502aa..c5903c41ed49 100644\n--- a/packages/react-server-dom-parcel/src/server/ReactFlightDOMServerNode.js\n+++ b/packages/react-server-dom-parcel/src/server/ReactFlightDOMServerNode.js\n@@ -556,12 +556,17 @@ export function registerServerActions(manifest: ServerManifest) {\n \n export function decodeReplyFromBusboy<T>(\n   busboyStream: Busboy,\n-  options?: {temporaryReferences?: TemporaryReferenceSet},\n+  options?: {\n+    temporaryReferences?: TemporaryReferenceSet,\n+    arraySizeLimit?: number,\n+  },\n ): Thenable<T> {\n   const response = createResponse(\n     serverManifest,\n     '',\n     options ? options.temporaryReferences : undefined,\n+    undefined,\n+    options ? options.arraySizeLimit : undefined,\n   );\n   let pendingFiles = 0;\n   const queuedFields: Array<string> = [];\n@@ -626,7 +631,10 @@ export function decodeReplyFromBusboy<T>(\n \n export function decodeReply<T>(\n   body: string | FormData,\n-  options?: {temporaryReferences?: TemporaryReferenceSet},\n+  options?: {\n+    temporaryReferences?: TemporaryReferenceSet,\n+    arraySizeLimit?: number,\n+  },\n ): Thenable<T> {\n   if (typeof body === 'string') {\n     const form = new FormData();\n@@ -638,6 +646,7 @@ export function decodeReply<T>(\n     '',\n     options ? options.temporaryReferences : undefined,\n     body,\n+    options ? options.arraySizeLimit : undefined,\n   );\n   const root = getRoot<T>(response);\n   close(response);\n@@ -646,7 +655,10 @@ export function decodeReply<T>(\n \n export function decodeReplyFromAsyncIterable<T>(\n   iterable: AsyncIterable<[string, string | File]>,\n-  options?: {temporaryReferences?: TemporaryReferenceSet},\n+  options?: {\n+    temporaryReferences?: TemporaryReferenceSet,\n+    arraySizeLimit?: number,\n+  },\n ): Thenable<T> {\n   const iterator: AsyncIterator<[string, string | File]> =\n     iterable[ASYNC_ITERATOR]();\n@@ -655,6 +667,8 @@ export function decodeReplyFromAsyncIterable<T>(\n     serverManifest,\n     '',\n     options ? options.temporaryReferences : undefined,\n+    undefined,\n+    options ? options.arraySizeLimit : undefined,\n   );\n \n   function progress(\ndiff --git a/packages/react-server-dom-turbopack/src/server/ReactFlightDOMServerBrowser.js b/packages/react-server-dom-turbopack/src/server/ReactFlightDOMServerBrowser.js\nindex 7ad138e62f5f..9388e5790f18 100644\n--- a/packages/react-server-dom-turbopack/src/server/ReactFlightDOMServerBrowser.js\n+++ b/packages/react-server-dom-turbopack/src/server/ReactFlightDOMServerBrowser.js\n@@ -239,7 +239,10 @@ function prerender(\n function decodeReply<T>(\n   body: string | FormData,\n   turbopackMap: ServerManifest,\n-  options?: {temporaryReferences?: TemporaryReferenceSet},\n+  options?: {\n+    temporaryReferences?: TemporaryReferenceSet,\n+    arraySizeLimit?: number,\n+  },\n ): Thenable<T> {\n   if (typeof body === 'string') {\n     const form = new FormData();\n@@ -251,6 +254,7 @@ function decodeReply<T>(\n     '',\n     options ? options.temporaryReferences : undefined,\n     body,\n+    options ? options.arraySizeLimit : undefined,\n   );\n   const root = getRoot<T>(response);\n   close(response);\ndiff --git a/packages/react-server-dom-turbopack/src/server/ReactFlightDOMServerEdge.js b/packages/react-server-dom-turbopack/src/server/ReactFlightDOMServerEdge.js\nindex 52b4468dcd3d..f6a8fcc9abcd 100644\n--- a/packages/react-server-dom-turbopack/src/server/ReactFlightDOMServerEdge.js\n+++ b/packages/react-server-dom-turbopack/src/server/ReactFlightDOMServerEdge.js\n@@ -244,7 +244,10 @@ function prerender(\n function decodeReply<T>(\n   body: string | FormData,\n   turbopackMap: ServerManifest,\n-  options?: {temporaryReferences?: TemporaryReferenceSet},\n+  options?: {\n+    temporaryReferences?: TemporaryReferenceSet,\n+    arraySizeLimit?: number,\n+  },\n ): Thenable<T> {\n   if (typeof body === 'string') {\n     const form = new FormData();\n@@ -256,6 +259,7 @@ function decodeReply<T>(\n     '',\n     options ? options.temporaryReferences : undefined,\n     body,\n+    options ? options.arraySizeLimit : undefined,\n   );\n   const root = getRoot<T>(response);\n   close(response);\n@@ -265,7 +269,10 @@ function decodeReply<T>(\n function decodeReplyFromAsyncIterable<T>(\n   iterable: AsyncIterable<[string, string | File]>,\n   turbopackMap: ServerManifest,\n-  options?: {temporaryReferences?: TemporaryReferenceSet},\n+  options?: {\n+    temporaryReferences?: TemporaryReferenceSet,\n+    arraySizeLimit?: number,\n+  },\n ): Thenable<T> {\n   const iterator: AsyncIterator<[string, string | File]> =\n     iterable[ASYNC_ITERATOR]();\n@@ -274,6 +281,8 @@ function decodeReplyFromAsyncIterable<T>(\n     turbopackMap,\n     '',\n     options ? options.temporaryReferences : undefined,\n+    undefined,\n+    options ? options.arraySizeLimit : undefined,\n   );\n \n   function progress(\ndiff --git a/packages/react-server-dom-turbopack/src/server/ReactFlightDOMServerNode.js b/packages/react-server-dom-turbopack/src/server/ReactFlightDOMServerNode.js\nindex 2f4301d1120f..74d379f53a02 100644\n--- a/packages/react-server-dom-turbopack/src/server/ReactFlightDOMServerNode.js\n+++ b/packages/react-server-dom-turbopack/src/server/ReactFlightDOMServerNode.js\n@@ -548,12 +548,17 @@ function prerender(\n function decodeReplyFromBusboy<T>(\n   busboyStream: Busboy,\n   turbopackMap: ServerManifest,\n-  options?: {temporaryReferences?: TemporaryReferenceSet},\n+  options?: {\n+    temporaryReferences?: TemporaryReferenceSet,\n+    arraySizeLimit?: number,\n+  },\n ): Thenable<T> {\n   const response = createResponse(\n     turbopackMap,\n     '',\n     options ? options.temporaryReferences : undefined,\n+    undefined,\n+    options ? options.arraySizeLimit : undefined,\n   );\n   let pendingFiles = 0;\n   const queuedFields: Array<string> = [];\n@@ -619,7 +624,10 @@ function decodeReplyFromBusboy<T>(\n function decodeReply<T>(\n   body: string | FormData,\n   turbopackMap: ServerManifest,\n-  options?: {temporaryReferences?: TemporaryReferenceSet},\n+  options?: {\n+    temporaryReferences?: TemporaryReferenceSet,\n+    arraySizeLimit?: number,\n+  },\n ): Thenable<T> {\n   if (typeof body === 'string') {\n     const form = new FormData();\n@@ -631,6 +639,7 @@ function decodeReply<T>(\n     '',\n     options ? options.temporaryReferences : undefined,\n     body,\n+    options ? options.arraySizeLimit : undefined,\n   );\n   const root = getRoot<T>(response);\n   close(response);\n@@ -640,7 +649,10 @@ function decodeReply<T>(\n function decodeReplyFromAsyncIterable<T>(\n   iterable: AsyncIterable<[string, string | File]>,\n   turbopackMap: ServerManifest,\n-  options?: {temporaryReferences?: TemporaryReferenceSet},\n+  options?: {\n+    temporaryReferences?: TemporaryReferenceSet,\n+    arraySizeLimit?: number,\n+  },\n ): Thenable<T> {\n   const iterator: AsyncIterator<[string, string | File]> =\n     iterable[ASYNC_ITERATOR]();\n@@ -649,6 +661,8 @@ function decodeReplyFromAsyncIterable<T>(\n     turbopackMap,\n     '',\n     options ? options.temporaryReferences : undefined,\n+    undefined,\n+    options ? options.arraySizeLimit : undefined,\n   );\n \n   function progress(\ndiff --git a/packages/react-server-dom-unbundled/src/server/ReactFlightDOMServerNode.js b/packages/react-server-dom-unbundled/src/server/ReactFlightDOMServerNode.js\nindex 647498a65d81..9a75c20395bc 100644\n--- a/packages/react-server-dom-unbundled/src/server/ReactFlightDOMServerNode.js\n+++ b/packages/react-server-dom-unbundled/src/server/ReactFlightDOMServerNode.js\n@@ -548,12 +548,17 @@ function prerender(\n function decodeReplyFromBusboy<T>(\n   busboyStream: Busboy,\n   webpackMap: ServerManifest,\n-  options?: {temporaryReferences?: TemporaryReferenceSet},\n+  options?: {\n+    temporaryReferences?: TemporaryReferenceSet,\n+    arraySizeLimit?: number,\n+  },\n ): Thenable<T> {\n   const response = createResponse(\n     webpackMap,\n     '',\n     options ? options.temporaryReferences : undefined,\n+    undefined,\n+    options ? options.arraySizeLimit : undefined,\n   );\n   let pendingFiles = 0;\n   const queuedFields: Array<string> = [];\n@@ -619,7 +624,10 @@ function decodeReplyFromBusboy<T>(\n function decodeReply<T>(\n   body: string | FormData,\n   webpackMap: ServerManifest,\n-  options?: {temporaryReferences?: TemporaryReferenceSet},\n+  options?: {\n+    temporaryReferences?: TemporaryReferenceSet,\n+    arraySizeLimit?: number,\n+  },\n ): Thenable<T> {\n   if (typeof body === 'string') {\n     const form = new FormData();\n@@ -631,6 +639,7 @@ function decodeReply<T>(\n     '',\n     options ? options.temporaryReferences : undefined,\n     body,\n+    options ? options.arraySizeLimit : undefined,\n   );\n   const root = getRoot<T>(response);\n   close(response);\n@@ -640,7 +649,10 @@ function decodeReply<T>(\n function decodeReplyFromAsyncIterable<T>(\n   iterable: AsyncIterable<[string, string | File]>,\n   webpackMap: ServerManifest,\n-  options?: {temporaryReferences?: TemporaryReferenceSet},\n+  options?: {\n+    temporaryReferences?: TemporaryReferenceSet,\n+    arraySizeLimit?: number,\n+  },\n ): Thenable<T> {\n   const iterator: AsyncIterator<[string, string | File]> =\n     iterable[ASYNC_ITERATOR]();\n@@ -649,6 +661,8 @@ function decodeReplyFromAsyncIterable<T>(\n     webpackMap,\n     '',\n     options ? options.temporaryReferences : undefined,\n+    undefined,\n+    options ? options.arraySizeLimit : undefined,\n   );\n \n   function progress(\ndiff --git a/packages/react-server-dom-webpack/src/server/ReactFlightDOMServerBrowser.js b/packages/react-server-dom-webpack/src/server/ReactFlightDOMServerBrowser.js\nindex 08192fd1e52d..1c417ff6bda3 100644\n--- a/packages/react-server-dom-webpack/src/server/ReactFlightDOMServerBrowser.js\n+++ b/packages/react-server-dom-webpack/src/server/ReactFlightDOMServerBrowser.js\n@@ -239,7 +239,10 @@ function prerender(\n function decodeReply<T>(\n   body: string | FormData,\n   webpackMap: ServerManifest,\n-  options?: {temporaryReferences?: TemporaryReferenceSet},\n+  options?: {\n+    temporaryReferences?: TemporaryReferenceSet,\n+    arraySizeLimit?: number,\n+  },\n ): Thenable<T> {\n   if (typeof body === 'string') {\n     const form = new FormData();\n@@ -251,6 +254,7 @@ function decodeReply<T>(\n     '',\n     options ? options.temporaryReferences : undefined,\n     body,\n+    options ? options.arraySizeLimit : undefined,\n   );\n   const root = getRoot<T>(response);\n   close(response);\ndiff --git a/packages/react-server-dom-webpack/src/server/ReactFlightDOMServerEdge.js b/packages/react-server-dom-webpack/src/server/ReactFlightDOMServerEdge.js\nindex 73fc74d4fa4f..77067754bc59 100644\n--- a/packages/react-server-dom-webpack/src/server/ReactFlightDOMServerEdge.js\n+++ b/packages/react-server-dom-webpack/src/server/ReactFlightDOMServerEdge.js\n@@ -244,7 +244,10 @@ function prerender(\n function decodeReply<T>(\n   body: string | FormData,\n   webpackMap: ServerManifest,\n-  options?: {temporaryReferences?: TemporaryReferenceSet},\n+  options?: {\n+    temporaryReferences?: TemporaryReferenceSet,\n+    arraySizeLimit?: number,\n+  },\n ): Thenable<T> {\n   if (typeof body === 'string') {\n     const form = new FormData();\n@@ -256,6 +259,7 @@ function decodeReply<T>(\n     '',\n     options ? options.temporaryReferences : undefined,\n     body,\n+    options ? options.arraySizeLimit : undefined,\n   );\n   const root = getRoot<T>(response);\n   close(response);\n@@ -265,7 +269,10 @@ function decodeReply<T>(\n function decodeReplyFromAsyncIterable<T>(\n   iterable: AsyncIterable<[string, string | File]>,\n   webpackMap: ServerManifest,\n-  options?: {temporaryReferences?: TemporaryReferenceSet},\n+  options?: {\n+    temporaryReferences?: TemporaryReferenceSet,\n+    arraySizeLimit?: number,\n+  },\n ): Thenable<T> {\n   const iterator: AsyncIterator<[string, string | File]> =\n     iterable[ASYNC_ITERATOR]();\n@@ -274,6 +281,8 @@ function decodeReplyFromAsyncIterable<T>(\n     webpackMap,\n     '',\n     options ? options.temporaryReferences : undefined,\n+    undefined,\n+    options ? options.arraySizeLimit : undefined,\n   );\n \n   function progress(\ndiff --git a/packages/react-server-dom-webpack/src/server/ReactFlightDOMServerNode.js b/packages/react-server-dom-webpack/src/server/ReactFlightDOMServerNode.js\nindex 5e73d8eb3a5d..888d01391449 100644\n--- a/packages/react-server-dom-webpack/src/server/ReactFlightDOMServerNode.js\n+++ b/packages/react-server-dom-webpack/src/server/ReactFlightDOMServerNode.js\n@@ -548,12 +548,17 @@ function prerender(\n function decodeReplyFromBusboy<T>(\n   busboyStream: Busboy,\n   webpackMap: ServerManifest,\n-  options?: {temporaryReferences?: TemporaryReferenceSet},\n+  options?: {\n+    temporaryReferences?: TemporaryReferenceSet,\n+    arraySizeLimit?: number,\n+  },\n ): Thenable<T> {\n   const response = createResponse(\n     webpackMap,\n     '',\n     options ? options.temporaryReferences : undefined,\n+    undefined,\n+    options ? options.arraySizeLimit : undefined,\n   );\n   let pendingFiles = 0;\n   const queuedFields: Array<string> = [];\n@@ -619,7 +624,10 @@ function decodeReplyFromBusboy<T>(\n function decodeReply<T>(\n   body: string | FormData,\n   webpackMap: ServerManifest,\n-  options?: {temporaryReferences?: TemporaryReferenceSet},\n+  options?: {\n+    temporaryReferences?: TemporaryReferenceSet,\n+    arraySizeLimit?: number,\n+  },\n ): Thenable<T> {\n   if (typeof body === 'string') {\n     const form = new FormData();\n@@ -631,6 +639,7 @@ function decodeReply<T>(\n     '',\n     options ? options.temporaryReferences : undefined,\n     body,\n+    options ? options.arraySizeLimit : undefined,\n   );\n   const root = getRoot<T>(response);\n   close(response);\n@@ -640,7 +649,10 @@ function decodeReply<T>(\n function decodeReplyFromAsyncIterable<T>(\n   iterable: AsyncIterable<[string, string | File]>,\n   webpackMap: ServerManifest,\n-  options?: {temporaryReferences?: TemporaryReferenceSet},\n+  options?: {\n+    temporaryReferences?: TemporaryReferenceSet,\n+    arraySizeLimit?: number,\n+  },\n ): Thenable<T> {\n   const iterator: AsyncIterator<[string, string | File]> =\n     iterable[ASYNC_ITERATOR]();\n@@ -649,6 +661,8 @@ function decodeReplyFromAsyncIterable<T>(\n     webpackMap,\n     '',\n     options ? options.temporaryReferences : undefined,\n+    undefined,\n+    options ? options.arraySizeLimit : undefined,\n   );\n \n   function progress(\ndiff --git a/packages/react-server/src/ReactFlightActionServer.js b/packages/react-server/src/ReactFlightActionServer.js\nindex 9062a6c03da4..ddcf36e96f99 100644\n--- a/packages/react-server/src/ReactFlightActionServer.js\n+++ b/packages/react-server/src/ReactFlightActionServer.js\n@@ -7,7 +7,7 @@\n  * @flow\n  */\n \n-import type {Thenable, ReactFormState} from 'shared/ReactTypes';\n+import type {ReactFormState} from 'shared/ReactTypes';\n \n import type {\n   ServerManifest,\n@@ -20,26 +20,48 @@ import {\n   requireModule,\n } from 'react-client/src/ReactFlightClientConfig';\n \n-import {createResponse, close, getRoot} from './ReactFlightReplyServer';\n+import {\n+  createResponse,\n+  close,\n+  getRoot,\n+  MAX_BOUND_ARGS,\n+} from './ReactFlightReplyServer';\n \n type ServerReferenceId = any;\n \n function bindArgs(fn: any, args: any) {\n+  if (args.length > MAX_BOUND_ARGS) {\n+    throw new Error(\n+      'Server Function has too many bound arguments. Received ' +\n+        args.length +\n+        ' but the limit is ' +\n+        MAX_BOUND_ARGS +\n+        '.',\n+    );\n+  }\n+\n   return fn.bind.apply(fn, [null].concat(args));\n }\n \n function loadServerReference<T>(\n   bundlerConfig: ServerManifest,\n-  id: ServerReferenceId,\n-  bound: null | Thenable<Array<any>>,\n+  metaData: {\n+    id: string,\n+    bound: null | Promise<Array<any>>,\n+  },\n ): Promise<T> {\n+  const id: ServerReferenceId = metaData.id;\n+  if (typeof id !== 'string') {\n+    return (null: any);\n+  }\n   const serverReference: ServerReference<T> =\n     resolveServerReference<$FlowFixMe>(bundlerConfig, id);\n   // We expect most servers to not really need this because you'd just have all\n   // the relevant modules already loaded but it allows for lazy loading of code\n   // if needed.\n   const preloadPromise = preloadModule(serverReference);\n-  if (bound) {\n+  const bound = metaData.bound;\n+  if (bound instanceof Promise) {\n     return Promise.all([(bound: any), preloadPromise]).then(\n       ([args]: Array<any>) => bindArgs(requireModule(serverReference), args),\n     );\n@@ -57,6 +79,7 @@ function decodeBoundActionMetaData(\n   body: FormData,\n   serverManifest: ServerManifest,\n   formFieldPrefix: string,\n+  arraySizeLimit: void | number,\n ): {id: ServerReferenceId, bound: null | Promise<Array<any>>} {\n   // The data for this reference is encoded in multiple fields under this prefix.\n   const actionResponse = createResponse(\n@@ -64,6 +87,7 @@ function decodeBoundActionMetaData(\n     formFieldPrefix,\n     undefined,\n     body,\n+    arraySizeLimit,\n   );\n   close(actionResponse);\n   const refPromise = getRoot<{\n@@ -89,6 +113,7 @@ export function decodeAction<T>(\n   const formData = new FormData();\n \n   let action: Promise<(formData: FormData) => T> | null = null;\n+  const seenActions = new Set<string>();\n \n   // $FlowFixMe[prop-missing]\n   body.forEach((value: string | File, key: string) => {\n@@ -97,21 +122,36 @@ export function decodeAction<T>(\n       formData.append(key, value);\n       return;\n     }\n-    // Later actions may override earlier actions if a button is used to override the default\n-    // form action.\n+    // Later actions may override earlier actions if a button is used to\n+    // override the default form action. However, we don't expect the same\n+    // action ref field to be sent multiple times in legitimate form data.\n     if (key.startsWith('$ACTION_REF_')) {\n+      if (seenActions.has(key)) {\n+        return;\n+      }\n+      seenActions.add(key);\n       const formFieldPrefix = '$ACTION_' + key.slice(12) + ':';\n       const metaData = decodeBoundActionMetaData(\n         body,\n         serverManifest,\n         formFieldPrefix,\n       );\n-      action = loadServerReference(serverManifest, metaData.id, metaData.bound);\n+      action = loadServerReference(serverManifest, metaData);\n       return;\n     }\n+    // A simple action with no bound arguments may appear twice in the form data\n+    // if a button specifies the same action as the default form action. We only\n+    // load the first one, as they're guaranteed to be identical.\n     if (key.startsWith('$ACTION_ID_')) {\n+      if (seenActions.has(key)) {\n+        return;\n+      }\n+      seenActions.add(key);\n       const id = key.slice(11);\n-      action = loadServerReference(serverManifest, id, null);\n+      action = loadServerReference(serverManifest, {\n+        id,\n+        bound: null,\n+      });\n       return;\n     }\n   });\ndiff --git a/packages/react-server/src/ReactFlightReplyServer.js b/packages/react-server/src/ReactFlightReplyServer.js\nindex 88781cc6eb8b..d3eff13ff465 100644\n--- a/packages/react-server/src/ReactFlightReplyServer.js\n+++ b/packages/react-server/src/ReactFlightReplyServer.js\n@@ -34,6 +34,7 @@ import {ASYNC_ITERATOR} from 'shared/ReactSymbols';\n \n import hasOwnProperty from 'shared/hasOwnProperty';\n import getPrototypeOf from 'shared/getPrototypeOf';\n+import isArray from 'shared/isArray';\n \n interface FlightStreamController {\n   enqueueModel(json: string): void;\n@@ -55,6 +56,8 @@ const RESOLVED_MODEL = 'resolved_model';\n const INITIALIZED = 'fulfilled';\n const ERRORED = 'rejected';\n \n+const __PROTO__ = '__proto__';\n+\n type RESPONSE_SYMBOL_TYPE = 'RESPONSE_SYMBOL'; // Fake symbol type.\n const RESPONSE_SYMBOL: RESPONSE_SYMBOL_TYPE = (Symbol(): any);\n \n@@ -79,7 +82,7 @@ type ResolvedModelChunk<T> = {\n type InitializedChunk<T> = {\n   status: 'fulfilled',\n   value: T,\n-  reason: null,\n+  reason: null | NestedArrayContext,\n   then(resolve: (T) => mixed, reject?: (mixed) => mixed): void,\n };\n type InitializedStreamChunk<\n@@ -194,6 +197,8 @@ export type Response = {\n   _closed: boolean,\n   _closedReason: mixed,\n   _temporaryReferences: void | TemporaryReferenceSet,\n+  _rootArrayContexts: WeakMap<$ReadOnlyArray<mixed>, NestedArrayContext>,\n+  _arraySizeLimit: number,\n };\n \n export function getRoot<T>(response: Response): Thenable<T> {\n@@ -210,13 +215,14 @@ function wakeChunk<T>(\n   response: Response,\n   listeners: Array<InitializationReference | (T => mixed)>,\n   value: T,\n+  chunk: InitializedChunk<T>,\n ): void {\n   for (let i = 0; i < listeners.length; i++) {\n     const listener = listeners[i];\n     if (typeof listener === 'function') {\n       listener(value);\n     } else {\n-      fulfillReference(response, listener, value);\n+      fulfillReference(response, listener, value, chunk.reason);\n     }\n   }\n }\n@@ -236,33 +242,6 @@ function rejectChunk(\n   }\n }\n \n-function resolveBlockedCycle<T>(\n-  resolvedChunk: SomeChunk<T>,\n-  reference: InitializationReference,\n-): null | InitializationHandler {\n-  const referencedChunk = reference.handler.chunk;\n-  if (referencedChunk === null) {\n-    return null;\n-  }\n-  if (referencedChunk === resolvedChunk) {\n-    // We found the cycle. We can resolve the blocked cycle now.\n-    return reference.handler;\n-  }\n-  const resolveListeners = referencedChunk.value;\n-  if (resolveListeners !== null) {\n-    for (let i = 0; i < resolveListeners.length; i++) {\n-      const listener = resolveListeners[i];\n-      if (typeof listener !== 'function') {\n-        const foundHandler = resolveBlockedCycle(resolvedChunk, listener);\n-        if (foundHandler !== null) {\n-          return foundHandler;\n-        }\n-      }\n-    }\n-  }\n-  return null;\n-}\n-\n function wakeChunkIfInitialized<T>(\n   response: Response,\n   chunk: SomeChunk<T>,\n@@ -271,45 +250,9 @@ function wakeChunkIfInitialized<T>(\n ): void {\n   switch (chunk.status) {\n     case INITIALIZED:\n-      wakeChunk(response, resolveListeners, chunk.value);\n+      wakeChunk(response, resolveListeners, chunk.value, chunk);\n       break;\n     case BLOCKED:\n-      // It is possible that we're blocked on our own chunk if it's a cycle.\n-      // Before adding back the listeners to the chunk, let's check if it would\n-      // result in a cycle.\n-      for (let i = 0; i < resolveListeners.length; i++) {\n-        const listener = resolveListeners[i];\n-        if (typeof listener !== 'function') {\n-          const reference: InitializationReference = listener;\n-          const cyclicHandler = resolveBlockedCycle(chunk, reference);\n-          if (cyclicHandler !== null) {\n-            // This reference points back to this chunk. We can resolve the cycle by\n-            // using the value from that handler.\n-            fulfillReference(response, reference, cyclicHandler.value);\n-            resolveListeners.splice(i, 1);\n-            i--;\n-            if (rejectListeners !== null) {\n-              const rejectionIdx = rejectListeners.indexOf(reference);\n-              if (rejectionIdx !== -1) {\n-                rejectListeners.splice(rejectionIdx, 1);\n-              }\n-            }\n-            // The status might have changed after fulfilling the reference.\n-            switch ((chunk: SomeChunk<T>).status) {\n-              case INITIALIZED:\n-                const initializedChunk: InitializedChunk<T> = (chunk: any);\n-                wakeChunk(response, resolveListeners, initializedChunk.value);\n-                return;\n-              case ERRORED:\n-                if (rejectListeners !== null) {\n-                  rejectChunk(response, rejectListeners, chunk.reason);\n-                }\n-                return;\n-            }\n-          }\n-        }\n-      }\n-    // Fallthrough\n     case PENDING:\n       if (chunk.value) {\n         for (let i = 0; i < resolveListeners.length; i++) {\n@@ -331,7 +274,7 @@ function wakeChunkIfInitialized<T>(\n       break;\n     case ERRORED:\n       if (rejectListeners) {\n-        wakeChunk(response, rejectListeners, chunk.reason);\n+        rejectChunk(response, rejectListeners, chunk.reason);\n       }\n       break;\n   }\n@@ -472,22 +415,73 @@ function loadServerReference<A: Iterable<any>, T>(\n     // as \"thenable\" which reduces to ReactPromise with no other fields.\n     return (null: any);\n   }\n+\n+  // Check for a cached promise from a previous call with the same metadata.\n+  // This handles deduplication when the same server reference appears multiple\n+  // times in the payload.\n+  const cachedPromise: SomeChunk<T> | void = (metaData: any).$$promise;\n+  if (cachedPromise !== undefined) {\n+    if (cachedPromise.status === INITIALIZED) {\n+      // The value was already resolved by a previous call.\n+      const resolvedValue: T = cachedPromise.value;\n+      if (key === __PROTO__) {\n+        return (null: any);\n+      }\n+      parentObject[key] = resolvedValue;\n+      return (resolvedValue: any);\n+    }\n+\n+    // The promise is still blocked. Increment the handler dependency count ...\n+    let handler: InitializationHandler;\n+    if (initializingHandler) {\n+      handler = initializingHandler;\n+      handler.deps++;\n+    } else {\n+      handler = initializingHandler = {\n+        chunk: null,\n+        value: null,\n+        reason: null,\n+        deps: 1,\n+        errored: false,\n+      };\n+    }\n+    // ... and register resolve and reject listeners on the promise.\n+    cachedPromise.then(\n+      resolveReference.bind(null, response, handler, parentObject, key),\n+      rejectReference.bind(null, response, handler),\n+    );\n+\n+    // Return a place holder value for now.\n+    return (null: any);\n+  }\n+\n+  // This is the first call for this server reference metadata. Create a cached\n+  // promise to be used for subsequent calls.\n+  // $FlowFixMe[invalid-constructor] Flow doesn't support functions as constructors\n+  const blockedPromise: BlockedChunk<T> = new ReactPromise(BLOCKED, null, null);\n+  (metaData: any).$$promise = blockedPromise;\n+\n   const serverReference: ServerReference<T> =\n     resolveServerReference<$FlowFixMe>(response._bundlerConfig, id);\n   // We expect most servers to not really need this because you'd just have all\n   // the relevant modules already loaded but it allows for lazy loading of code\n   // if needed.\n   const bound = metaData.bound;\n-  let promise: null | Thenable<any> = preloadModule(serverReference);\n-  if (!promise) {\n+  let serverReferencePromise: null | Thenable<any> =\n+    preloadModule(serverReference);\n+  if (!serverReferencePromise) {\n     if (bound instanceof ReactPromise) {\n-      promise = Promise.resolve(bound);\n+      serverReferencePromise = Promise.resolve(bound);\n     } else {\n       const resolvedValue = (requireModule(serverReference): any);\n+      // Resolve the cached promise synchronously.\n+      const initializedPromise: InitializedChunk<T> = (blockedPromise: any);\n+      initializedPromise.status = INITIALIZED;\n+      initializedPromise.value = resolvedValue;\n       return resolvedValue;\n     }\n   } else if (bound instanceof ReactPromise) {\n-    promise = Promise.all([promise, bound]);\n+    serverReferencePromise = Promise.all([serverReferencePromise, bound]);\n   }\n \n   let handler: InitializationHandler;\n@@ -508,59 +502,59 @@ function loadServerReference<A: Iterable<any>, T>(\n     let resolvedValue = (requireModule(serverReference): any);\n \n     if (metaData.bound) {\n-      // This promise is coming from us and should have initilialized by now.\n+      // This promise is coming from us and should have initialized by now.\n       const promiseValue = (metaData.bound: any).value;\n-      const boundArgs: Array<any> = Array.isArray(promiseValue)\n+      const boundArgs: Array<any> = isArray(promiseValue)\n         ? promiseValue.slice(0)\n         : [];\n+      if (boundArgs.length > MAX_BOUND_ARGS) {\n+        reject(\n+          new Error(\n+            'Server Function has too many bound arguments. Received ' +\n+              boundArgs.length +\n+              ' but the limit is ' +\n+              MAX_BOUND_ARGS +\n+              '.',\n+          ),\n+        );\n+        return;\n+      }\n       boundArgs.unshift(null); // this\n       resolvedValue = resolvedValue.bind.apply(resolvedValue, boundArgs);\n     }\n \n-    parentObject[key] = resolvedValue;\n-\n-    // If this is the root object for a model reference, where `handler.value`\n-    // is a stale `null`, the resolved value can be used directly.\n-    if (key === '' && handler.value === null) {\n-      handler.value = resolvedValue;\n+    // Resolve the cached promise so subsequent references can use the value.\n+    const resolveListeners = blockedPromise.value;\n+    const initializedPromise: InitializedChunk<T> = (blockedPromise: any);\n+    initializedPromise.status = INITIALIZED;\n+    initializedPromise.value = resolvedValue;\n+    initializedPromise.reason = null;\n+    if (resolveListeners !== null) {\n+      // Notify any resolve listeners that were added via .then() from\n+      // subsequent loadServerReference calls for the same reference.\n+      wakeChunk(response, resolveListeners, resolvedValue, initializedPromise);\n     }\n \n-    handler.deps--;\n-\n-    if (handler.deps === 0) {\n-      const chunk = handler.chunk;\n-      if (chunk === null || chunk.status !== BLOCKED) {\n-        return;\n-      }\n-      const resolveListeners = chunk.value;\n-      const initializedChunk: InitializedChunk<T> = (chunk: any);\n-      initializedChunk.status = INITIALIZED;\n-      initializedChunk.value = handler.value;\n-      initializedChunk.reason = null;\n-      if (resolveListeners !== null) {\n-        wakeChunk(response, resolveListeners, handler.value);\n-      }\n-    }\n+    resolveReference(response, handler, parentObject, key, resolvedValue);\n   }\n \n   function reject(error: mixed): void {\n-    if (handler.errored) {\n-      // We've already errored. We could instead build up an AggregateError\n-      // but if there are multiple errors we just take the first one like\n-      // Promise.all.\n-      return;\n-    }\n-    handler.errored = true;\n-    handler.value = null;\n-    handler.reason = error;\n-    const chunk = handler.chunk;\n-    if (chunk === null || chunk.status !== BLOCKED) {\n-      return;\n+    // Mark the cached promise as errored so subsequent references fail too.\n+    const rejectListeners = blockedPromise.reason;\n+    const erroredPromise: ErroredChunk<T> = (blockedPromise: any);\n+    erroredPromise.status = ERRORED;\n+    erroredPromise.value = null;\n+    erroredPromise.reason = error;\n+    if (rejectListeners !== null) {\n+      // Notify any reject listeners that were added via .then() from subsequent\n+      // loadServerReference calls for the same reference.\n+      rejectChunk(response, rejectListeners, error);\n     }\n-    triggerErrorOnChunk(response, chunk, error);\n+\n+    rejectReference(response, handler, error);\n   }\n \n-  promise.then(fulfill, reject);\n+  serverReferencePromise.then(fulfill, reject);\n \n   // Return a place holder value for now.\n   return (null: any);\n@@ -572,10 +566,18 @@ function reviveModel(\n   parentKey: string,\n   value: JSONValue,\n   reference: void | string,\n+  arrayRoot: null | NestedArrayContext,\n ): any {\n   if (typeof value === 'string') {\n     // We can't use .bind here because we need the \"this\" value.\n-    return parseModelString(response, parentObj, parentKey, value, reference);\n+    return parseModelString(\n+      response,\n+      parentObj,\n+      parentKey,\n+      value,\n+      reference,\n+      arrayRoot,\n+    );\n   }\n   if (typeof value === 'object' && value !== null) {\n     if (\n@@ -589,16 +591,42 @@ function reviveModel(\n         reference,\n       );\n     }\n-    if (Array.isArray(value)) {\n+    if (isArray(value)) {\n+      let childContext: NestedArrayContext;\n+      if (arrayRoot === null) {\n+        childContext = ({\n+          count: 0,\n+          fork: false,\n+        }: NestedArrayContext);\n+        response._rootArrayContexts.set(value, childContext);\n+      } else {\n+        childContext = arrayRoot;\n+      }\n+      if (value.length > 1) {\n+        childContext.fork = true;\n+      }\n+      bumpArrayCount(childContext, value.length + 1, response);\n       for (let i = 0; i < value.length; i++) {\n         const childRef =\n           reference !== undefined ? reference + ':' + i : undefined;\n         // $FlowFixMe[cannot-write]\n-        value[i] = reviveModel(response, value, '' + i, value[i], childRef);\n+        value[i] = reviveModel(\n+          response,\n+          value,\n+          '' + i,\n+          value[i],\n+          childRef,\n+          childContext,\n+        );\n       }\n     } else {\n       for (const key in value) {\n         if (hasOwnProperty.call(value, key)) {\n+          if (key === __PROTO__) {\n+            // $FlowFixMe[cannot-write]\n+            delete value[key];\n+            continue;\n+          }\n           const childRef =\n             reference !== undefined && key.indexOf(':') === -1\n               ? reference + ':' + key\n@@ -609,8 +637,9 @@ function reviveModel(\n             key,\n             value[key],\n             childRef,\n+            null, // The array context resets when we're entering a non-array\n           );\n-          if (newValue !== undefined || key === '__proto__') {\n+          if (newValue !== undefined) {\n             // $FlowFixMe[cannot-write]\n             value[key] = newValue;\n           } else {\n@@ -624,6 +653,27 @@ function reviveModel(\n   return value;\n }\n \n+type NestedArrayContext = {\n+  // Keeps track of how many slots, bytes or characters are in nested arrays/strings/typed arrays.\n+  count: number,\n+  // A single child is itself not harmful. There needs to be at least one parent array with more\n+  // than one child.\n+  fork: boolean,\n+};\n+\n+function bumpArrayCount(\n+  arrayContext: NestedArrayContext,\n+  slots: number,\n+  response: Response,\n+): void {\n+  const newCount = (arrayContext.count += slots);\n+  if (newCount > response._arraySizeLimit && arrayContext.fork) {\n+    throw new Error(\n+      'Maximum array nesting exceeded. Large nested arrays can be dangerous. Try adding intermediate objects.',\n+    );\n+  }\n+}\n+\n type InitializationReference = {\n   handler: InitializationHandler,\n   parentObject: Object,\n@@ -635,6 +685,7 @@ type InitializationReference = {\n     key: string,\n   ) => any,\n   path: Array<string>,\n+  arrayRoot: null | NestedArrayContext,\n };\n type InitializationHandler = {\n   chunk: null | BlockedChunk<any>,\n@@ -666,12 +717,19 @@ function initializeModelChunk<T>(chunk: ResolvedModelChunk<T>): void {\n   try {\n     const rawModel = JSON.parse(resolvedModel);\n \n+    // The root might not be an array but if it is we want to track the count of entries.\n+    const arrayRoot: NestedArrayContext = {\n+      count: 0,\n+      fork: false,\n+    };\n+\n     const value: T = reviveModel(\n       response,\n       {'': rawModel},\n       '',\n       rawModel,\n       rootReference,\n+      arrayRoot,\n     );\n \n     // Invoke any listeners added while resolving this model. I.e. cyclic\n@@ -686,7 +744,7 @@ function initializeModelChunk<T>(chunk: ResolvedModelChunk<T>): void {\n         if (typeof listener === 'function') {\n           listener(value);\n         } else {\n-          fulfillReference(response, listener, value);\n+          fulfillReference(response, listener, value, arrayRoot);\n         }\n       }\n     }\n@@ -698,6 +756,7 @@ function initializeModelChunk<T>(chunk: ResolvedModelChunk<T>): void {\n         // We discovered new dependencies on modules that are not yet resolved.\n         // We have to keep the BLOCKED state until they're resolved.\n         initializingHandler.value = value;\n+        initializingHandler.reason = arrayRoot;\n         initializingHandler.chunk = cyclicChunk;\n         return;\n       }\n@@ -705,7 +764,7 @@ function initializeModelChunk<T>(chunk: ResolvedModelChunk<T>): void {\n     const initializedChunk: InitializedChunk<T> = (chunk: any);\n     initializedChunk.status = INITIALIZED;\n     initializedChunk.value = value;\n-    initializedChunk.reason = null;\n+    initializedChunk.reason = arrayRoot;\n   } catch (error) {\n     const erroredChunk: ErroredChunk<T> = (chunk: any);\n     erroredChunk.status = ERRORED;\n@@ -727,7 +786,11 @@ export function reportGlobalError(response: Response, error: Error): void {\n     if (chunk.status === PENDING) {\n       triggerErrorOnChunk(response, chunk, error);\n     } else if (chunk.status === INITIALIZED && chunk.reason !== null) {\n-      chunk.reason.error(error);\n+      const maybeController = chunk.reason;\n+      // $FlowFixMe\n+      if (typeof maybeController.error === 'function') {\n+        maybeController.error(error);\n+      }\n     }\n   });\n }\n@@ -759,10 +822,14 @@ function fulfillReference(\n   response: Response,\n   reference: InitializationReference,\n   value: any,\n+  arrayRoot: null | NestedArrayContext,\n ): void {\n   const {handler, parentObject, key, map, path} = reference;\n \n+  let resolvedValue;\n   try {\n+    let localLength: number = 0;\n+    const rootArrayContexts = response._rootArrayContexts;\n     for (let i = 1; i < path.length; i++) {\n       // The server doesn't have any lazy references so we don't expect to go through a Promise.\n       const name = path[i];\n@@ -774,26 +841,77 @@ function fulfillReference(\n         hasOwnProperty.call(value, name)\n       ) {\n         value = value[name];\n+        if (isArray(value)) {\n+          localLength = 0;\n+          arrayRoot = rootArrayContexts.get(value) || arrayRoot;\n+        } else {\n+          arrayRoot = null;\n+          if (typeof value === 'string') {\n+            localLength = value.length;\n+          } else if (typeof value === 'bigint') {\n+            // Estimate the length to avoid expensive toString() calls on large\n+            // BigInt values. If the value is too large, we get Infinity, which\n+            // will trigger the array size limit error.\n+            // eslint-disable-next-line react-internal/no-primitive-constructors\n+            const n = Math.abs(Number(value));\n+            if (n === 0) {\n+              localLength = 1;\n+            } else {\n+              localLength = Math.floor(Math.log10(n)) + 1;\n+            }\n+          } else if (ArrayBuffer.isView(value)) {\n+            localLength = value.byteLength;\n+          } else {\n+            localLength = 0;\n+          }\n+        }\n       } else {\n         throw new Error('Invalid reference.');\n       }\n     }\n \n-    const mappedValue = map(response, value, parentObject, key);\n-    parentObject[key] = mappedValue;\n+    resolvedValue = map(response, value, parentObject, key);\n \n-    // If this is the root object for a model reference, where `handler.value`\n-    // is a stale `null`, the resolved value can be used directly.\n-    if (key === '' && handler.value === null) {\n-      handler.value = mappedValue;\n+    // Add any array counts to the reference's array root. The value that we're\n+    // resolving might have deep nesting that we need to resolve.\n+    const referenceArrayRoot = reference.arrayRoot;\n+    if (referenceArrayRoot !== null) {\n+      if (arrayRoot !== null) {\n+        if (arrayRoot.fork) {\n+          referenceArrayRoot.fork = true;\n+        }\n+        bumpArrayCount(referenceArrayRoot, arrayRoot.count, response);\n+      } else if (localLength > 0) {\n+        bumpArrayCount(referenceArrayRoot, localLength, response);\n+      }\n     }\n   } catch (error) {\n-    rejectReference(response, reference.handler, error);\n+    rejectReference(response, handler, error);\n     return;\n   }\n \n   // There are no Elements or Debug Info to transfer here.\n \n+  resolveReference(response, handler, parentObject, key, resolvedValue);\n+}\n+\n+function resolveReference(\n+  response: Response,\n+  handler: InitializationHandler,\n+  parentObject: Object,\n+  key: string,\n+  resolvedValue: mixed,\n+): void {\n+  if (key !== __PROTO__) {\n+    parentObject[key] = resolvedValue;\n+  }\n+\n+  // If this is the root object for a model reference, where `handler.value`\n+  // is a stale `null`, the resolved value can be used directly.\n+  if (key === '' && handler.value === null) {\n+    handler.value = resolvedValue;\n+  }\n+\n   handler.deps--;\n \n   if (handler.deps === 0) {\n@@ -807,7 +925,7 @@ function fulfillReference(\n     initializedChunk.value = handler.value;\n     initializedChunk.reason = handler.reason; // Used by streaming chunks\n     if (resolveListeners !== null) {\n-      wakeChunk(response, resolveListeners, handler.value);\n+      wakeChunk(response, resolveListeners, handler.value, initializedChunk);\n     }\n   }\n }\n@@ -835,10 +953,11 @@ function rejectReference(\n }\n \n function waitForReference<T>(\n-  referencedChunk: PendingChunk<T> | BlockedChunk<T>,\n+  response: Response,\n+  referencedChunk: BlockedChunk<T>,\n   parentObject: Object,\n   key: string,\n-  response: Response,\n+  arrayRoot: null | NestedArrayContext,\n   map: (response: Response, model: any, parentObject: Object, key: string) => T,\n   path: Array<string>,\n ): T {\n@@ -862,6 +981,7 @@ function waitForReference<T>(\n     key,\n     map,\n     path,\n+    arrayRoot,\n   };\n \n   // Add \"listener\".\n@@ -885,6 +1005,7 @@ function getOutlinedModel<T>(\n   reference: string,\n   parentObject: Object,\n   key: string,\n+  referenceArrayRoot: null | NestedArrayContext,\n   map: (response: Response, model: any, parentObject: Object, key: string) => T,\n ): T {\n   const path = reference.split(':');\n@@ -899,6 +1020,9 @@ function getOutlinedModel<T>(\n   switch (chunk.status) {\n     case INITIALIZED:\n       let value = chunk.value;\n+      let arrayRoot: null | NestedArrayContext = chunk.reason;\n+      let localLength: number = 0;\n+      const rootArrayContexts = response._rootArrayContexts;\n       for (let i = 1; i < path.length; i++) {\n         const name = path[i];\n         if (\n@@ -909,16 +1033,64 @@ function getOutlinedModel<T>(\n           hasOwnProperty.call(value, name)\n         ) {\n           value = value[name];\n+          if (isArray(value)) {\n+            localLength = 0;\n+            arrayRoot = rootArrayContexts.get(value) || arrayRoot;\n+          } else {\n+            arrayRoot = null;\n+            if (typeof value === 'string') {\n+              localLength = value.length;\n+            } else if (typeof value === 'bigint') {\n+              // Estimate the length to avoid expensive toString() calls on large\n+              // BigInt values. If the value is too large, we get Infinity, which\n+              // will trigger the array size limit error.\n+              // eslint-disable-next-line react-internal/no-primitive-constructors\n+              const n = Math.abs(Number(value));\n+              if (n === 0) {\n+                localLength = 1;\n+              } else {\n+                localLength = Math.floor(Math.log10(n)) + 1;\n+              }\n+            } else if (ArrayBuffer.isView(value)) {\n+              localLength = value.byteLength;\n+            } else {\n+              localLength = 0;\n+            }\n+          }\n         } else {\n           throw new Error('Invalid reference.');\n         }\n       }\n       const chunkValue = map(response, value, parentObject, key);\n+\n+      // Add any array counts to the reference's array root. The value that we're\n+      // resolving might have deep nesting that we need to resolve.\n+      if (referenceArrayRoot !== null) {\n+        if (arrayRoot !== null) {\n+          if (arrayRoot.fork) {\n+            referenceArrayRoot.fork = true;\n+          }\n+          bumpArrayCount(referenceArrayRoot, arrayRoot.count, response);\n+        } else if (localLength > 0) {\n+          bumpArrayCount(referenceArrayRoot, localLength, response);\n+        }\n+      }\n       // There's no Element nor Debug Info in the ReplyServer so we don't have to check those here.\n       return chunkValue;\n-    case PENDING:\n     case BLOCKED:\n-      return waitForReference(chunk, parentObject, key, response, map, path);\n+      return waitForReference(\n+        response,\n+        chunk,\n+        parentObject,\n+        key,\n+        referenceArrayRoot,\n+        map,\n+        path,\n+      );\n+    case PENDING:\n+      // If we don't have the referenced chunk yet, then this must be a forward reference,\n+      // which is not allowed.\n+      throw new Error('Invalid forward reference.');\n     default:\n       // This is an error. Instead of erroring directly, we're going to encode this on\n       // an initialization handler.\n@@ -944,16 +1116,40 @@ function createMap(\n   response: Response,\n   model: Array<[any, any]>,\n ): Map<any, any> {\n-  return new Map(model);\n+  if (!isArray(model)) {\n+    throw new Error('Invalid Map initializer.');\n+  }\n+  if ((model as any).$$consumed === true) {\n+    throw new Error('Already initialized Map.');\n+  }\n+  const map = new Map(model);\n+  (model as any).$$consumed = true;\n+  return map;\n }\n \n function createSet(response: Response, model: Array<any>): Set<any> {\n-  return new Set(model);\n+  if (!isArray(model)) {\n+    throw new Error('Invalid Set initializer.');\n+  }\n+  if ((model as any).$$consumed === true) {\n+    throw new Error('Already initialized Set.');\n+  }\n+  const set = new Set(model);\n+  (model as any).$$consumed = true;\n+  return set;\n }\n \n function extractIterator(response: Response, model: Array<any>): Iterator<any> {\n+  if (!isArray(model)) {\n+    throw new Error('Invalid Iterator initializer.');\n+  }\n+  if ((model as any).$$consumed === true) {\n+    throw new Error('Already initialized Iterator.');\n+  }\n   // $FlowFixMe[incompatible-use]: This uses raw Symbols because we're extracting from a native array.\n-  return model[Symbol.iterator]();\n+  const iterator = model[Symbol.iterator]();\n+  (model as any).$$consumed = true;\n+  return iterator;\n }\n \n function createModel(\n@@ -977,6 +1173,7 @@ function parseTypedArray<T: $ArrayBufferView | ArrayBuffer>(\n   bytesPerElement: number,\n   parentObject: Object,\n   parentKey: string,\n+  referenceArrayRoot: null | NestedArrayContext,\n ): null {\n   const id = parseInt(reference.slice(2), 16);\n   const prefix = response._prefix;\n@@ -985,10 +1182,15 @@ function parseTypedArray<T: $ArrayBufferView | ArrayBuffer>(\n   if (chunks.has(id)) {\n     throw new Error('Already initialized typed array.');\n   }\n+  chunks.set(\n+    id,\n+    // We don't need to put the actual Blob in the chunk,\n+    // because it shouldn't be accessed by anything else.\n+    createErroredChunk(response, new Error('Already initialized typed array.')),\n+  );\n \n   // We should have this backingEntry in the store already because we emitted\n   // it before referencing it. It should be a Blob.\n-  // TODO: Use getOutlinedModel to allow us to emit the Blob later. We should be able to do that now.\n   const backingEntry: Blob = (response._formData.get(key): any);\n \n   const promise: Promise<ArrayBuffer> = backingEntry.arrayBuffer();\n@@ -1011,17 +1213,28 @@ function parseTypedArray<T: $ArrayBufferView | ArrayBuffer>(\n   }\n \n   function fulfill(buffer: ArrayBuffer): void {\n-    const resolvedValue: T =\n-      constructor === ArrayBuffer\n-        ? (buffer: any)\n-        : (new constructor(buffer): any);\n+    try {\n+      if (referenceArrayRoot !== null) {\n+        bumpArrayCount(referenceArrayRoot, buffer.byteLength, response);\n+      }\n \n-    parentObject[parentKey] = resolvedValue;\n+      const resolvedValue: T =\n+        constructor === ArrayBuffer\n+          ? (buffer: any)\n+          : (new constructor(buffer): any);\n \n-    // If this is the root object for a model reference, where `handler.value`\n-    // is a stale `null`, the resolved value can be used directly.\n-    if (parentKey === '' && handler.value === null) {\n-      handler.value = resolvedValue;\n+      if (key !== __PROTO__) {\n+        parentObject[parentKey] = resolvedValue;\n+      }\n+\n+      // If this is the root object for a model reference, where `handler.value`\n+      // is a stale `null`, the resolved value can be used directly.\n+      if (parentKey === '' && handler.value === null) {\n+        handler.value = resolvedValue;\n+      }\n+    } catch (x) {\n+      reject(x);\n+      return;\n     }\n \n     handler.deps--;\n@@ -1035,9 +1248,11 @@ function parseTypedArray<T: $ArrayBufferView | ArrayBuffer>(\n       const initializedChunk: InitializedChunk<T> = (chunk: any);\n       initializedChunk.status = INITIALIZED;\n       initializedChunk.value = handler.value;\n+      // We don't keep an array count for this since it won't be referenced again.\n+      // In fact, we don't really need to store this chunk at all.\n       initializedChunk.reason = null;\n       if (resolveListeners !== null) {\n-        wakeChunk(response, resolveListeners, handler.value);\n+        wakeChunk(response, resolveListeners, handler.value, initializedChunk);\n       }\n     }\n   }\n@@ -1111,6 +1326,13 @@ function parseReadableStream<T>(\n     },\n   });\n   let previousBlockedChunk: SomeChunk<T> | null = null;\n+  function enqueue(value: T): void {\n+    if (type === 'bytes' && !ArrayBuffer.isView(value)) {\n+      flightController.error(new Error('Invalid data for bytes stream.'));\n+      return;\n+    }\n+    controller.enqueue(value);\n+  }\n   const flightController = {\n     enqueueModel(json: string): void {\n       if (previousBlockedChunk === null) {\n@@ -1124,22 +1346,16 @@ function parseReadableStream<T>(\n         initializeModelChunk(chunk);\n         const initializedChunk: SomeChunk<T> = chunk;\n         if (initializedChunk.status === INITIALIZED) {\n-          controller.enqueue(initializedChunk.value);\n+          enqueue(initializedChunk.value);\n         } else {\n-          chunk.then(\n-            v => controller.enqueue(v),\n-            e => controller.error((e: any)),\n-          );\n+          chunk.then(enqueue, flightController.error);\n           previousBlockedChunk = chunk;\n         }\n       } else {\n         // We're still waiting on a previous chunk so we can't enqueue quite yet.\n         const blockedChunk = previousBlockedChunk;\n         const chunk: SomeChunk<T> = createPendingChunk(response);\n-        chunk.then(\n-          v => controller.enqueue(v),\n-          e => controller.error((e: any)),\n-        );\n+        chunk.then(enqueue, flightController.error);\n         previousBlockedChunk = chunk;\n         blockedChunk.then(function () {\n           if (previousBlockedChunk === chunk) {\n@@ -1185,24 +1401,23 @@ function parseReadableStream<T>(\n   return stream;\n }\n \n-function asyncIterator(this: $AsyncIterator<any, any, void>) {\n+function FlightIterator(\n+  this: {next: (arg: void) => SomeChunk<IteratorResult<any, any>>, ...},\n+  next: (arg: void) => SomeChunk<IteratorResult<any, any>>,\n+) {\n+  this.next = next;\n+  // TODO: Add return/throw as options for aborting.\n+}\n+// TODO: The iterator could inherit the AsyncIterator prototype which is not exposed as\n+// a global but exists as a prototype of an AsyncGenerator. However, it's not needed\n+// to satisfy the iterable protocol.\n+FlightIterator.prototype = ({}: any);\n+FlightIterator.prototype[ASYNC_ITERATOR] = function asyncIterator(\n+  this: $AsyncIterator<any, any, void>,\n+) {\n   // Self referencing iterator.\n   return this;\n-}\n-\n-function createIterator<T>(\n-  next: (arg: void) => SomeChunk<IteratorResult<T, T>>,\n-): $AsyncIterator<T, T, void> {\n-  const iterator: any = {\n-    next: next,\n-    // TODO: Add return/throw as options for aborting.\n-  };\n-  // TODO: The iterator could inherit the AsyncIterator prototype which is not exposed as\n-  // a global but exists as a prototype of an AsyncGenerator. However, it's not needed\n-  // to satisfy the iterable protocol.\n-  (iterator: any)[ASYNC_ITERATOR] = asyncIterator;\n-  return iterator;\n-}\n+};\n \n function parseAsyncIterable<T>(\n   response: Response,\n@@ -1285,7 +1500,8 @@ function parseAsyncIterable<T>(\n   const iterable: $AsyncIterable<T, T, void> = {\n     [ASYNC_ITERATOR](): $AsyncIterator<T, T, void> {\n       let nextReadIndex = 0;\n-      return createIterator(arg => {\n+      // $FlowFixMe[invalid-constructor] Flow doesn't support functions as constructors\n+      return new FlightIterator((arg: void) => {\n         if (arg !== undefined) {\n           throw new Error(\n             'Values cannot be passed to next() of AsyncIterables passed to Client Components.',\n@@ -1320,11 +1536,15 @@ function parseModelString(\n   key: string,\n   value: string,\n   reference: void | string,\n+  arrayRoot: null | NestedArrayContext,\n ): any {\n   if (value[0] === '$') {\n     switch (value[1]) {\n       case '$': {\n         // This was an escaped string value.\n+        if (arrayRoot !== null) {\n+          bumpArrayCount(arrayRoot, value.length - 1, response);\n+        }\n         return value.slice(1);\n       }\n       case '@': {\n@@ -1336,7 +1556,14 @@ function parseModelString(\n       case 'h': {\n         // Server Reference\n         const ref = value.slice(2);\n-        return getOutlinedModel(response, ref, obj, key, loadServerReference);\n+        return getOutlinedModel(\n+          response,\n+          ref,\n+          obj,\n+          key,\n+          null,\n+          loadServerReference,\n+        );\n       }\n       case 'T': {\n         // Temporary Reference\n@@ -1358,12 +1585,12 @@ function parseModelString(\n       case 'Q': {\n         // Map\n         const ref = value.slice(2);\n-        return getOutlinedModel(response, ref, obj, key, createMap);\n+        return getOutlinedModel(response, ref, obj, key, null, createMap);\n       }\n       case 'W': {\n         // Set\n         const ref = value.slice(2);\n-        return getOutlinedModel(response, ref, obj, key, createSet);\n+        return getOutlinedModel(response, ref, obj, key, null, createSet);\n       }\n       case 'K': {\n         // FormData\n@@ -1374,19 +1601,30 @@ function parseModelString(\n         // We assume that the reference to FormData always comes after each\n         // entry that it references so we can assume they all exist in the\n         // backing store already.\n-        // $FlowFixMe[prop-missing] FormData has forEach on it.\n-        backingFormData.forEach((entry: File | string, entryKey: string) => {\n+        // Clone the keys to workaround bugs in the delete-while-iterating\n+        // algorithm of FormData.\n+        const keys = Array.from(backingFormData.keys());\n+        for (let i = 0; i < keys.length; i++) {\n+          const entryKey = keys[i];\n           if (entryKey.startsWith(formPrefix)) {\n-            // $FlowFixMe[incompatible-call]\n-            data.append(entryKey.slice(formPrefix.length), entry);\n+            const entries = backingFormData.getAll(entryKey);\n+            const newKey = entryKey.slice(formPrefix.length);\n+            for (let j = 0; j < entries.length; j++) {\n+              // $FlowFixMe[incompatible-call]\n+              data.append(newKey, entries[j]);\n+            }\n+            // These entries have now all been consumed. Let's free it.\n+            // This also ensures that we don't have any entries left if we\n+            // see the same key twice.\n+            backingFormData.delete(entryKey);\n           }\n-        });\n+        }\n         return data;\n       }\n       case 'i': {\n         // Iterator\n         const ref = value.slice(2);\n-        return getOutlinedModel(response, ref, obj, key, extractIterator);\n+        return getOutlinedModel(response, ref, obj, key, null, extractIterator);\n       }\n       case 'I': {\n         // $Infinity\n@@ -1415,36 +1653,151 @@ function parseModelString(\n       }\n       case 'n': {\n         // BigInt\n-        return BigInt(value.slice(2));\n+        const bigIntStr = value.slice(2);\n+        if (bigIntStr.length > MAX_BIGINT_DIGITS) {\n+          throw new Error(\n+            'BigInt is too large. Received ' +\n+              bigIntStr.length +\n+              ' digits but the limit is ' +\n+              MAX_BIGINT_DIGITS +\n+              '.',\n+          );\n+        }\n+        if (arrayRoot !== null) {\n+          bumpArrayCount(arrayRoot, bigIntStr.length, response);\n+        }\n+        return BigInt(bigIntStr);\n       }\n-    }\n-    switch (value[1]) {\n       case 'A':\n-        return parseTypedArray(response, value, ArrayBuffer, 1, obj, key);\n+        return parseTypedArray(\n+          response,\n+          value,\n+          ArrayBuffer,\n+          1,\n+          obj,\n+          key,\n+          arrayRoot,\n+        );\n       case 'O':\n-        return parseTypedArray(response, value, Int8Array, 1, obj, key);\n+        return parseTypedArray(\n+          response,\n+          value,\n+          Int8Array,\n+          1,\n+          obj,\n+          key,\n+          arrayRoot,\n+        );\n       case 'o':\n-        return parseTypedArray(response, value, Uint8Array, 1, obj, key);\n+        return parseTypedArray(\n+          response,\n+          value,\n+          Uint8Array,\n+          1,\n+          obj,\n+          key,\n+          arrayRoot,\n+        );\n       case 'U':\n-        return parseTypedArray(response, value, Uint8ClampedArray, 1, obj, key);\n+        return parseTypedArray(\n+          response,\n+          value,\n+          Uint8ClampedArray,\n+          1,\n+          obj,\n+          key,\n+          arrayRoot,\n+        );\n       case 'S':\n-        return parseTypedArray(response, value, Int16Array, 2, obj, key);\n+        return parseTypedArray(\n+          response,\n+          value,\n+          Int16Array,\n+          2,\n+          obj,\n+          key,\n+          arrayRoot,\n+        );\n       case 's':\n-        return parseTypedArray(response, value, Uint16Array, 2, obj, key);\n+        return parseTypedArray(\n+          response,\n+          value,\n+          Uint16Array,\n+          2,\n+          obj,\n+          key,\n+          arrayRoot,\n+        );\n       case 'L':\n-        return parseTypedArray(response, value, Int32Array, 4, obj, key);\n+        return parseTypedArray(\n+          response,\n+          value,\n+          Int32Array,\n+          4,\n+          obj,\n+          key,\n+          arrayRoot,\n+        );\n       case 'l':\n-        return parseTypedArray(response, value, Uint32Array, 4, obj, key);\n+        return parseTypedArray(\n+          response,\n+          value,\n+          Uint32Array,\n+          4,\n+          obj,\n+          key,\n+          arrayRoot,\n+        );\n       case 'G':\n-        return parseTypedArray(response, value, Float32Array, 4, obj, key);\n+        return parseTypedArray(\n+          response,\n+          value,\n+          Float32Array,\n+          4,\n+          obj,\n+          key,\n+          arrayRoot,\n+        );\n       case 'g':\n-        return parseTypedArray(response, value, Float64Array, 8, obj, key);\n+        return parseTypedArray(\n+          response,\n+          value,\n+          Float64Array,\n+          8,\n+          obj,\n+          key,\n+          arrayRoot,\n+        );\n       case 'M':\n-        return parseTypedArray(response, value, BigInt64Array, 8, obj, key);\n+        return parseTypedArray(\n+          response,\n+          value,\n+          BigInt64Array,\n+          8,\n+          obj,\n+          key,\n+          arrayRoot,\n+        );\n       case 'm':\n-        return parseTypedArray(response, value, BigUint64Array, 8, obj, key);\n+        return parseTypedArray(\n+          response,\n+          value,\n+          BigUint64Array,\n+          8,\n+          obj,\n+          key,\n+          arrayRoot,\n+        );\n       case 'V':\n-        return parseTypedArray(response, value, DataView, 1, obj, key);\n+        return parseTypedArray(\n+          response,\n+          value,\n+          DataView,\n+          1,\n+          obj,\n+          key,\n+          arrayRoot,\n+        );\n       case 'B': {\n         // Blob\n         const id = parseInt(value.slice(2), 16);\n@@ -1455,8 +1808,6 @@ function parseModelString(\n         const backingEntry: Blob = (response._formData.get(blobKey): any);\n         return backingEntry;\n       }\n-    }\n-    switch (value[1]) {\n       case 'R': {\n         return parseReadableStream(response, value, undefined, obj, key);\n       }\n@@ -1472,16 +1823,30 @@ function parseModelString(\n     }\n     // We assume that anything else is a reference ID.\n     const ref = value.slice(1);\n-    return getOutlinedModel(response, ref, obj, key, createModel);\n+    return getOutlinedModel(response, ref, obj, key, arrayRoot, createModel);\n+  }\n+  if (arrayRoot !== null) {\n+    bumpArrayCount(arrayRoot, value.length, response);\n   }\n   return value;\n }\n \n+const DEFAULT_MAX_ARRAY_NESTING = 1000000;\n+\n+// Limit BigInt size to prevent CPU exhaustion from parsing very large values.\n+// 300 digits covers most practical use cases (even 512-bit integers need only\n+// ~154 digits) and aligns with the implicit limit from the Number approximation\n+// checks in fulfillReference and getOutlinedModel.\n+const MAX_BIGINT_DIGITS = 300;\n+\n+export const MAX_BOUND_ARGS = 1000;\n+\n export function createResponse(\n   bundlerConfig: ServerManifest,\n   formFieldPrefix: string,\n   temporaryReferences: void | TemporaryReferenceSet,\n   backingFormData?: FormData = new FormData(),\n+  arraySizeLimit?: number = DEFAULT_MAX_ARRAY_NESTING,\n ): Response {\n   const chunks: Map<number, SomeChunk<any>> = new Map();\n   const response: Response = {\n@@ -1492,6 +1857,8 @@ export function createResponse(\n     _closed: false,\n     _closedReason: null,\n     _temporaryReferences: temporaryReferences,\n+    _rootArrayContexts: new WeakMap(),\n+    _arraySizeLimit: arraySizeLimit,\n   };\n   return response;\n }\ndiff --git a/packages/react-server/src/ReactFlightServer.js b/packages/react-server/src/ReactFlightServer.js\nindex 1f9bb0a77fad..f31fa45f7a77 100644\n--- a/packages/react-server/src/ReactFlightServer.js\n+++ b/packages/react-server/src/ReactFlightServer.js\n@@ -557,6 +557,8 @@ type DeferredDebugStore = {\n   existing: Map<ReactClientReference | string, number>,\n };\n \n+const __PROTO__ = '__proto__';\n+\n const OPENING = 10;\n const OPEN = 11;\n const ABORTING = 12;\n@@ -3447,6 +3449,17 @@ function renderModelDestructive(\n   // Set the currently rendering model\n   task.model = value;\n \n+  if (__DEV__) {\n+    if (parentPropertyName === __PROTO__) {\n+      callWithDebugContextInDEV(request, task, () => {\n+        console.error(\n+          'Expected not to serialize an object with own property `__proto__`. When parsed this property will be omitted.%s',\n+          describeObjectForErrorMessage(parent, parentPropertyName),\n+        );\n+      });\n+    }\n+  }\n+\n   // Special Symbol, that's very common.\n   if (value === REACT_ELEMENT_TYPE) {\n     return '$';\ndiff --git a/scripts/error-codes/codes.json b/scripts/error-codes/codes.json\nindex d8c8e0b7685c..46bb2e6bccf0 100644\n--- a/scripts/error-codes/codes.json\n+++ b/scripts/error-codes/codes.json\n@@ -555,5 +555,16 @@\n   \"567\": \"Already initialized stream.\",\n   \"568\": \"Already initialized typed array.\",\n   \"569\": \"Cannot have cyclic thenables.\",\n-  \"570\": \"Invalid reference.\"\n+  \"570\": \"Invalid reference.\",\n+  \"571\": \"Maximum array nesting exceeded. Large nested arrays can be dangerous. Try adding intermediate objects.\",\n+  \"572\": \"Already initialized Map.\",\n+  \"573\": \"Already initialized Set.\",\n+  \"574\": \"Invalid forward reference.\",\n+  \"575\": \"Invalid Map initializer.\",\n+  \"576\": \"Invalid Set initializer.\",\n+  \"577\": \"Invalid Iterator initializer.\",\n+  \"578\": \"Already initialized Iterator.\",\n+  \"579\": \"Invalid data for bytes stream.\",\n+  \"580\": \"Server Function has too many bound arguments. Received %s but the limit is %s.\",\n+  \"581\": \"BigInt is too large. Received %s digits but the limit is %s.\"\n }\n",
			"results": [
				{
					"modelId": "anthropic/claude-opus-4.5",
					"tier": "frontier",
					"status": "success",
					"message": "fix(react-server): prevent prototype pollution and add security limits for server actions",
					"latencyMs": 2588,
					"costUsd": 0.12221,
					"inputTokens": 24347,
					"outputTokens": 19,
					"humanReview": {
						"overallScore": 3,
						"notes": "Pending manual review.",
						"winnerFlag": false
					},
					"providerMetadata": {
						"anthropic": {
							"usage": {
								"input_tokens": 24347,
								"output_tokens": 19,
								"cache_creation_input_tokens": 0,
								"cache_read_input_tokens": 0,
								"cache_creation": {
									"ephemeral_5m_input_tokens": 0,
									"ephemeral_1h_input_tokens": 0
								},
								"service_tier": "standard",
								"inference_geo": "not_available"
							},
							"cacheCreationInputTokens": 0,
							"stopSequence": null,
							"iterations": null,
							"container": null,
							"contextManagement": null
						},
						"gateway": {
							"routing": {
								"originalModelId": "anthropic/claude-opus-4.5",
								"resolvedProvider": "anthropic",
								"resolvedProviderApiModelId": "claude-opus-4-5-20251101",
								"internalResolvedModelId": "anthropic:claude-opus-4-5-20251101",
								"fallbacksAvailable": ["vertexAnthropic", "bedrock"],
								"internalReasoning": "Selected anthropic as preferred provider for claude-opus-4.5. 2 fallback(s) available: vertexAnthropic, bedrock",
								"planningReasoning": "BYOK credentials available for: anthropic, vertexAnthropic. System credentials planned for: anthropic, vertexAnthropic, bedrock. Total execution order: anthropic(BYOK)  vertexAnthropic(BYOK)  anthropic(system)  vertexAnthropic(system)  bedrock(system)",
								"canonicalSlug": "anthropic/claude-opus-4.5",
								"finalProvider": "anthropic",
								"attempts": [
									{
										"provider": "anthropic",
										"internalModelId": "anthropic:claude-opus-4-5-20251101",
										"providerApiModelId": "claude-opus-4-5-20251101",
										"credentialType": "byok",
										"success": true,
										"startTime": 628650.901921,
										"endTime": 630727.003336,
										"statusCode": 200,
										"providerResponseId": "msg_01H5YwvcnGPpTAa2ysPF3hEu"
									}
								],
								"modelAttemptCount": 1,
								"modelAttempts": [
									{
										"modelId": "anthropic/claude-opus-4.5",
										"canonicalSlug": "anthropic/claude-opus-4.5",
										"success": false,
										"providerAttemptCount": 0,
										"providerAttempts": []
									}
								],
								"totalProviderAttemptCount": 0
							},
							"cost": "0",
							"marketCost": "0.12221",
							"generationId": "gen_01KJ3S4DNA4HRRTK8DKSTSST00",
							"billableWebSearchCalls": 0
						}
					}
				},
				{
					"modelId": "cerebras/llama3.1-8b",
					"tier": "small",
					"status": "error",
					"message": "",
					"latencyMs": 1182,
					"costUsd": null,
					"inputTokens": 0,
					"outputTokens": 0,
					"humanReview": {
						"overallScore": 3,
						"notes": "Pending manual review.",
						"winnerFlag": false
					},
					"providerMetadata": null,
					"errorMessage": "Please reduce the length of the messages or completion. Current length is 18510 while limit is 8192"
				},
				{
					"modelId": "google/gemini-3-pro-preview",
					"tier": "frontier",
					"status": "success",
					"message": "fix(",
					"latencyMs": 9352,
					"costUsd": 0.046246,
					"inputTokens": 22427,
					"outputTokens": 116,
					"humanReview": {
						"overallScore": 3,
						"notes": "Pending manual review.",
						"winnerFlag": false
					},
					"providerMetadata": {
						"vertex": {
							"promptFeedback": null,
							"groundingMetadata": null,
							"urlContextMetadata": null,
							"safetyRatings": null,
							"usageMetadata": {
								"thoughtsTokenCount": 114,
								"promptTokenCount": 22427,
								"candidatesTokenCount": 2,
								"totalTokenCount": 22543,
								"trafficType": "ON_DEMAND"
							}
						},
						"gateway": {
							"routing": {
								"originalModelId": "google/gemini-3-pro-preview",
								"resolvedProvider": "vertex",
								"resolvedProviderApiModelId": "gemini-3-pro-preview",
								"internalResolvedModelId": "vertex:gemini-3-pro-preview",
								"fallbacksAvailable": ["google"],
								"internalReasoning": "Selected vertex as preferred provider for gemini-3-pro-preview. 1 fallback(s) available: google",
								"planningReasoning": "BYOK credentials available for: vertex. System credentials planned for: vertex, google. Total execution order: vertex(BYOK)  vertex(system)  google(system)",
								"canonicalSlug": "google/gemini-3-pro-preview",
								"finalProvider": "vertex",
								"attempts": [
									{
										"provider": "vertex",
										"internalModelId": "vertex:gemini-3-pro-preview",
										"providerApiModelId": "gemini-3-pro-preview",
										"credentialType": "byok",
										"success": true,
										"startTime": 2318046.296368,
										"endTime": 2326736.075957,
										"statusCode": 200,
										"providerResponseId": "y4mbaYD7BsC28eEPsLG5mQs"
									}
								],
								"modelAttemptCount": 1,
								"modelAttempts": [
									{
										"modelId": "google/gemini-3-pro-preview",
										"canonicalSlug": "google/gemini-3-pro-preview",
										"success": false,
										"providerAttemptCount": 0,
										"providerAttempts": []
									}
								],
								"totalProviderAttemptCount": 0
							},
							"cost": "0",
							"marketCost": "0.046246",
							"generationId": "gen_01KJ3S4G5CKMX6HG5BEVC9WNZW",
							"billableWebSearchCalls": 0
						}
					}
				},
				{
					"modelId": "mistral/ministral-3b",
					"tier": "small",
					"status": "success",
					"message": "feat:server-flight add array size limit and proto property handling",
					"latencyMs": 1310,
					"costUsd": 0.00079372,
					"inputTokens": 19828,
					"outputTokens": 15,
					"humanReview": {
						"overallScore": 3,
						"notes": "Pending manual review.",
						"winnerFlag": false
					},
					"providerMetadata": {
						"gateway": {
							"routing": {
								"originalModelId": "mistral/ministral-3b",
								"resolvedProvider": "mistral",
								"resolvedProviderApiModelId": "ministral-3b-latest",
								"internalResolvedModelId": "mistral:ministral-3b-latest",
								"fallbacksAvailable": [],
								"internalReasoning": "Selected mistral as preferred provider for ministral-3b. 0 fallback(s) available: ",
								"planningReasoning": "System credentials planned for: mistral. Total execution order: mistral(system)",
								"canonicalSlug": "mistral/ministral-3b",
								"finalProvider": "mistral",
								"attempts": [
									{
										"provider": "mistral",
										"internalModelId": "mistral:ministral-3b-latest",
										"providerApiModelId": "ministral-3b-latest",
										"credentialType": "system",
										"success": true,
										"startTime": 65313.823228,
										"endTime": 66001.658282,
										"statusCode": 200
									}
								],
								"modelAttemptCount": 1,
								"modelAttempts": [
									{
										"modelId": "mistral/ministral-3b",
										"canonicalSlug": "mistral/ministral-3b",
										"success": false,
										"providerAttemptCount": 0,
										"providerAttempts": []
									}
								],
								"totalProviderAttemptCount": 0
							},
							"cost": "0.00079372",
							"marketCost": "0.00079372",
							"generationId": "gen_01KJ3S4CFNFW1F8YYC214FXXGQ",
							"billableWebSearchCalls": 0
						}
					}
				},
				{
					"modelId": "openai/gpt-5.2",
					"tier": "frontier",
					"status": "success",
					"message": "fix(flight): harden reply parsing against __proto__ and resource exhaustion",
					"latencyMs": 2548,
					"costUsd": 0.032739,
					"inputTokens": 18556,
					"outputTokens": 19,
					"humanReview": {
						"overallScore": 3,
						"notes": "Pending manual review.",
						"winnerFlag": false
					},
					"providerMetadata": {
						"openai": {
							"responseId": "resp_01538d6b9bde483b00699b89c8f1d0819e907f5f83bdc533fc",
							"serviceTier": "default"
						},
						"gateway": {
							"routing": {
								"originalModelId": "openai/gpt-5.2",
								"resolvedProvider": "openai",
								"resolvedProviderApiModelId": "gpt-5.2",
								"internalResolvedModelId": "openai:gpt-5.2",
								"fallbacksAvailable": ["azure"],
								"internalReasoning": "Selected openai as preferred provider for gpt-5.2. 1 fallback(s) available: azure",
								"planningReasoning": "BYOK credentials available for: openai. System credentials planned for: openai, azure. Total execution order: openai(BYOK)  openai(system)  azure(system)",
								"canonicalSlug": "openai/gpt-5.2",
								"finalProvider": "openai",
								"attempts": [
									{
										"provider": "openai",
										"internalModelId": "openai:gpt-5.2",
										"providerApiModelId": "gpt-5.2",
										"credentialType": "byok",
										"success": true,
										"startTime": 628695.859907,
										"endTime": 630557.98545,
										"statusCode": 200,
										"providerResponseId": "resp_01538d6b9bde483b00699b89c8f1d0819e907f5f83bdc533fc"
									}
								],
								"modelAttemptCount": 1,
								"modelAttempts": [
									{
										"modelId": "openai/gpt-5.2",
										"canonicalSlug": "openai/gpt-5.2",
										"success": false,
										"providerAttemptCount": 0,
										"providerAttempts": []
									}
								],
								"totalProviderAttemptCount": 0
							},
							"cost": "0",
							"marketCost": "0.032739",
							"generationId": "gen_01KJ3S4DP321JX6K3W085A4CG1",
							"billableWebSearchCalls": 0
						}
					}
				},
				{
					"modelId": "xai/grok-code-fast-1",
					"tier": "small",
					"status": "success",
					"message": "feat(flight): add security protections against prototype pollution and DoS attacks",
					"latencyMs": 22787,
					"costUsd": 0.00473408,
					"inputTokens": 18646,
					"outputTokens": 716,
					"humanReview": {
						"overallScore": 3,
						"notes": "Pending manual review.",
						"winnerFlag": false
					},
					"providerMetadata": {
						"gateway": {
							"routing": {
								"originalModelId": "xai/grok-code-fast-1",
								"resolvedProvider": "xai",
								"resolvedProviderApiModelId": "grok-code-fast-1",
								"internalResolvedModelId": "xai:grok-code-fast-1",
								"fallbacksAvailable": [],
								"internalReasoning": "Selected xai as preferred provider for grok-code-fast-1. 0 fallback(s) available: ",
								"planningReasoning": "System credentials planned for: xai. Total execution order: xai(system)",
								"canonicalSlug": "xai/grok-code-fast-1",
								"finalProvider": "xai",
								"attempts": [
									{
										"provider": "xai",
										"internalModelId": "xai:grok-code-fast-1",
										"providerApiModelId": "grok-code-fast-1",
										"credentialType": "system",
										"success": true,
										"startTime": 2314795.951883,
										"endTime": 2336792.371616,
										"statusCode": 200
									}
								],
								"modelAttemptCount": 1,
								"modelAttempts": [
									{
										"modelId": "xai/grok-code-fast-1",
										"canonicalSlug": "xai/grok-code-fast-1",
										"success": false,
										"providerAttemptCount": 0,
										"providerAttempts": []
									}
								],
								"totalProviderAttemptCount": 0
							},
							"cost": "0.00473408",
							"marketCost": "0.00473408",
							"generationId": "gen_01KJ3S4CMGAGMQWV6EKEJGZCMJ",
							"billableWebSearchCalls": 0
						}
					}
				}
			]
		},
		{
			"id": "next-js-aa6d7c22-devtools-omit-empty-looking-err",
			"title": "[devtools] Omit empty looking error messages (#90256)",
			"sourceRepo": "vercel/next.js",
			"sourceCommitUrl": "https://github.com/vercel/next.js/commit/aa6d7c2271092417de894c2d492ea01253b8117f",
			"diff": "diff --git a/packages/next/src/next-devtools/dev-overlay/components/errors/error-message/error-message.tsx b/packages/next/src/next-devtools/dev-overlay/components/errors/error-message/error-message.tsx\nindex f2b98e05cf4e1..92c20dcafeb1d 100644\n--- a/packages/next/src/next-devtools/dev-overlay/components/errors/error-message/error-message.tsx\n+++ b/packages/next/src/next-devtools/dev-overlay/components/errors/error-message/error-message.tsx\n@@ -19,6 +19,10 @@ export function ErrorMessage({ errorMessage, errorType }: ErrorMessageProps) {\n     }\n   }, [errorMessage])\n \n+  if (!errorMessage) {\n+    return null\n+  }\n+\n   // The \"Blocking Route\" error message is specifically formatted to look nice\n   // in the overlay (rather than just passed through from the console), so we\n   // intentionally don't truncate it and rely on the scroll overflow instead.\ndiff --git a/packages/next/src/next-devtools/dev-overlay/container/errors.tsx b/packages/next/src/next-devtools/dev-overlay/container/errors.tsx\nindex 3768e40b061ee..7504d7ff28615 100644\n--- a/packages/next/src/next-devtools/dev-overlay/container/errors.tsx\n+++ b/packages/next/src/next-devtools/dev-overlay/container/errors.tsx\n@@ -55,6 +55,11 @@ function GenericErrorDescription({ error }: { error: Error }) {\n     message = message.slice(envPrefix.length)\n   }\n \n+  message = message.trim()\n+  if (!message) {\n+    return null\n+  }\n+\n   return (\n     <>\n       <HotlinkedText text={message} matcher={matchLinkType} />\ndiff --git a/packages/next/src/next-devtools/dev-overlay/container/runtime-error/error-cause.tsx b/packages/next/src/next-devtools/dev-overlay/container/runtime-error/error-cause.tsx\nindex 64c0ec6542e73..8169c24ca0e5d 100644\n--- a/packages/next/src/next-devtools/dev-overlay/container/runtime-error/error-cause.tsx\n+++ b/packages/next/src/next-devtools/dev-overlay/container/runtime-error/error-cause.tsx\n@@ -11,6 +11,7 @@ type ErrorCauseProps = {\n \n export function ErrorCause({ cause, dialogResizerRef }: ErrorCauseProps) {\n   const frames = React.use(cause.frames())\n+  const trimmedMessage = cause.error.message.trim()\n \n   const firstFrame = useMemo(() => {\n     const index = frames.findIndex(\n@@ -29,7 +30,9 @@ export function ErrorCause({ cause, dialogResizerRef }: ErrorCauseProps) {\n           Caused by: {cause.error.name || 'Error'}\n         </span>\n       </div>\n-      <p className=\"error-cause-message\">{cause.error.message}</p>\n+      {trimmedMessage ? (\n+        <p className=\"error-cause-message\">{trimmedMessage}</p>\n+      ) : null}\n \n       {firstFrame && (\n         <CodeFrame\ndiff --git a/test/e2e/app-dir/instant-validation-causes/instant-validation-causes.test.ts b/test/e2e/app-dir/instant-validation-causes/instant-validation-causes.test.ts\nindex 204bee76ef8c4..6cabdf194cd0e 100644\n--- a/test/e2e/app-dir/instant-validation-causes/instant-validation-causes.test.ts\n+++ b/test/e2e/app-dir/instant-validation-causes/instant-validation-causes.test.ts\n@@ -96,7 +96,6 @@ describe('instant validation causes', () => {\n        \"cause\": [\n          {\n            \"label\": \"Caused by: Instant Validation\",\n-           \"message\": \" \",\n            \"source\": \"app/named-export/page.tsx (3:26) @ unstable_instant\n      > 3 | const unstable_instant = { prefetch: 'static' }\n          |                          ^\",\n@@ -141,7 +140,6 @@ describe('instant validation causes', () => {\n        \"cause\": [\n          {\n            \"label\": \"Caused by: Instant Validation\",\n-           \"message\": \" \",\n            \"source\": \"app/aliased-export/page.tsx (3:17) @ unstable_instant\n      > 3 | const instant = { prefetch: 'static' }\n          |                 ^\",\n@@ -186,7 +184,6 @@ describe('instant validation causes', () => {\n        \"cause\": [\n          {\n            \"label\": \"Caused by: Instant Validation\",\n-           \"message\": \" \",\n            \"source\": \"app/reexport/page.tsx (3:10) @ unstable_instant\n      > 3 | export { unstable_instant } from './config'\n          |          ^\",\n@@ -234,7 +231,6 @@ describe('instant validation causes', () => {\n        \"cause\": [\n          {\n            \"label\": \"Caused by: Instant Validation\",\n-           \"message\": \" \",\n            \"source\": \"app/indirect-export/page.tsx (4:17) @ unstable_instant\n      > 4 | const instant = _instant\n          |                 ^\",\ndiff --git a/test/e2e/app-dir/instant-validation/instant-validation.test.ts b/test/e2e/app-dir/instant-validation/instant-validation.test.ts\nindex 877649192a3ec..aab515d4d4d6d 100644\n--- a/test/e2e/app-dir/instant-validation/instant-validation.test.ts\n+++ b/test/e2e/app-dir/instant-validation/instant-validation.test.ts\n@@ -189,7 +189,6 @@ describe('instant validation', () => {\n          \"cause\": [\n            {\n              \"label\": \"Caused by: Instant Validation\",\n-             \"message\": \" \",\n              \"source\": \"app/suspense-in-root/static/missing-suspense-around-runtime/page.tsx (3:33) @ unstable_instant\n        > 3 | export const unstable_instant = { prefetch: 'static' }\n            |                                 ^\",\n@@ -234,7 +233,6 @@ describe('instant validation', () => {\n          \"cause\": [\n            {\n              \"label\": \"Caused by: Instant Validation\",\n-             \"message\": \" \",\n              \"source\": \"app/suspense-in-root/static/missing-suspense-around-dynamic/page.tsx (3:33) @ unstable_instant\n        > 3 | export const unstable_instant = { prefetch: 'static' }\n            |                                 ^\",\n@@ -277,7 +275,6 @@ describe('instant validation', () => {\n          \"cause\": [\n            {\n              \"label\": \"Caused by: Instant Validation\",\n-             \"message\": \" \",\n              \"source\": \"app/suspense-in-root/runtime/missing-suspense-around-dynamic/page.tsx (4:33) @ unstable_instant\n        > 4 | export const unstable_instant = {\n            |                                 ^\",\n@@ -322,7 +319,6 @@ describe('instant validation', () => {\n          \"cause\": [\n            {\n              \"label\": \"Caused by: Instant Validation\",\n-             \"message\": \" \",\n              \"source\": \"app/suspense-in-root/static/missing-suspense-around-dynamic-layout/layout.tsx (4:33) @ unstable_instant\n        > 4 | export const unstable_instant = { prefetch: 'static' }\n            |                                 ^\",\n@@ -367,7 +363,6 @@ describe('instant validation', () => {\n          \"cause\": [\n            {\n              \"label\": \"Caused by: Instant Validation\",\n-             \"message\": \" \",\n              \"source\": \"app/suspense-in-root/runtime/missing-suspense-around-dynamic-layout/layout.tsx (4:33) @ unstable_instant\n        > 4 | export const unstable_instant = {\n            |                                 ^\",\n@@ -411,7 +406,6 @@ describe('instant validation', () => {\n          \"cause\": [\n            {\n              \"label\": \"Caused by: Instant Validation\",\n-             \"message\": \" \",\n              \"source\": \"app/suspense-in-root/static/missing-suspense-around-params/[param]/page.tsx (1:33) @ unstable_instant\n        > 1 | export const unstable_instant = { prefetch: 'static' }\n            |                                 ^\",\n@@ -465,7 +459,6 @@ describe('instant validation', () => {\n          \"cause\": [\n            {\n              \"label\": \"Caused by: Instant Validation\",\n-             \"message\": \" \",\n              \"source\": \"app/suspense-in-root/static/missing-suspense-around-search-params/page.tsx (1:33) @ unstable_instant\n        > 1 | export const unstable_instant = { prefetch: 'static' }\n            |                                 ^\",\n@@ -532,7 +525,6 @@ describe('instant validation', () => {\n          \"cause\": [\n            {\n              \"label\": \"Caused by: Instant Validation\",\n-             \"message\": \" \",\n              \"source\": \"app/suspense-in-root/static/suspense-too-high/page.tsx (3:33) @ unstable_instant\n        > 3 | export const unstable_instant = { prefetch: 'static' }\n            |                                 ^\",\n@@ -577,7 +569,6 @@ describe('instant validation', () => {\n          \"cause\": [\n            {\n              \"label\": \"Caused by: Instant Validation\",\n-             \"message\": \" \",\n              \"source\": \"app/suspense-in-root/runtime/suspense-too-high/page.tsx (4:33) @ unstable_instant\n        > 4 | export const unstable_instant = {\n            |                                 ^\",\n@@ -707,7 +698,6 @@ describe('instant validation', () => {\n          \"cause\": [\n            {\n              \"label\": \"Caused by: Instant Validation\",\n-             \"message\": \" \",\n              \"source\": \"app/suspense-in-root/static/invalid-only-loading-around-dynamic/page.tsx (4:33) @ unstable_instant\n        > 4 | export const unstable_instant = { prefetch: 'static' }\n            |                                 ^\",\n@@ -759,7 +749,6 @@ describe('instant validation', () => {\n            \"cause\": [\n              {\n                \"label\": \"Caused by: Instant Validation\",\n-               \"message\": \" \",\n                \"source\": \"app/suspense-in-root/static/blocking-layout/missing-suspense-around-dynamic/page.tsx (3:33) @ unstable_instant\n          > 3 | export const unstable_instant = { prefetch: 'static' }\n              |                                 ^\",\n@@ -818,7 +807,6 @@ describe('instant validation', () => {\n            \"cause\": [\n              {\n                \"label\": \"Caused by: Instant Validation\",\n-               \"message\": \" \",\n                \"source\": \"app/suspense-in-root/static/invalid-blocking-inside-static/layout.tsx (1:33) @ unstable_instant\n          > 1 | export const unstable_instant = { prefetch: 'static' }\n              |                                 ^\",\n@@ -863,7 +851,6 @@ describe('instant validation', () => {\n            \"cause\": [\n              {\n                \"label\": \"Caused by: Instant Validation\",\n-               \"message\": \" \",\n                \"source\": \"app/suspense-in-root/runtime/invalid-blocking-inside-runtime/layout.tsx (3:33) @ unstable_instant\n          > 3 | export const unstable_instant = {\n              |                                 ^\",\n@@ -909,7 +896,6 @@ describe('instant validation', () => {\n            \"cause\": [\n              {\n                \"label\": \"Caused by: Instant Validation\",\n-               \"message\": \" \",\n                \"source\": \"app/suspense-in-root/static/missing-suspense-in-parallel-route/page.tsx (3:33) @ unstable_instant\n          > 3 | export const unstable_instant = { prefetch: 'static' }\n              |                                 ^\",\n@@ -955,7 +941,6 @@ describe('instant validation', () => {\n            \"cause\": [\n              {\n                \"label\": \"Caused by: Instant Validation\",\n-               \"message\": \" \",\n                \"source\": \"app/suspense-in-root/static/missing-suspense-in-parallel-route/foo/page.tsx (1:33) @ unstable_instant\n          > 1 | export const unstable_instant = { prefetch: 'static' }\n              |                                 ^\",\n@@ -1001,7 +986,6 @@ describe('instant validation', () => {\n            \"cause\": [\n              {\n                \"label\": \"Caused by: Instant Validation\",\n-               \"message\": \" \",\n                \"source\": \"app/suspense-in-root/static/missing-suspense-in-parallel-route/bar/page.tsx (1:33) @ unstable_instant\n          > 1 | export const unstable_instant = { prefetch: 'static' }\n              |                                 ^\",\n@@ -1049,7 +1033,6 @@ describe('instant validation', () => {\n            \"cause\": [\n              {\n                \"label\": \"Caused by: Instant Validation\",\n-               \"message\": \" \",\n                \"source\": \"app/suspense-in-root/static/invalid-client-data-blocks-validation/page.tsx (1:33) @ unstable_instant\n          > 1 | export const unstable_instant = {\n              |                                 ^\",\ndiff --git a/test/lib/add-redbox-matchers.ts b/test/lib/add-redbox-matchers.ts\nindex eb72703f1827e..700037007e7e9 100644\n--- a/test/lib/add-redbox-matchers.ts\n+++ b/test/lib/add-redbox-matchers.ts\n@@ -75,7 +75,7 @@ interface ErrorSnapshotOptions {\n \n interface SanitizedCauseEntry {\n   label: string | null\n-  message: string | null\n+  message?: string\n   source: string | null\n   stack: string[]\n }\n@@ -83,7 +83,7 @@ interface SanitizedCauseEntry {\n export interface ErrorSnapshot {\n   environmentLabel: string | null\n   label: string | null\n-  description: string | null\n+  description?: string\n   componentStack?: string\n   cause?: SanitizedCauseEntry[]\n   source: string | null\n@@ -239,11 +239,14 @@ async function createErrorSnapshot(\n   const snapshot: ErrorSnapshot = {\n     environmentLabel,\n     label: label ?? '<FIXME-excluded-label>',\n-    description: sanitizedDescription,\n     source: focusedSource,\n     stack: sanitizeStack(stack, next),\n   }\n \n+  if (sanitizedDescription !== null) {\n+    snapshot.description = sanitizedDescription\n+  }\n+\n   // Hydration diffs are only relevant to some specific errors\n   // so we hide them from the snapshots unless they are present.\n   if (componentStack !== null) {\n@@ -252,12 +255,17 @@ async function createErrorSnapshot(\n \n   // Error.cause chain is only relevant when present.\n   if (cause !== null) {\n-    snapshot.cause = cause.map((entry) => ({\n-      label: entry.label,\n-      message: entry.message,\n-      source: focusSource(entry.source, next),\n-      stack: sanitizeStack(entry.stack, next) ?? [],\n-    }))\n+    snapshot.cause = cause.map((entry) => {\n+      const causeEntry: SanitizedCauseEntry = {\n+        label: entry.label,\n+        source: focusSource(entry.source, next),\n+        stack: sanitizeStack(entry.stack, next) ?? [],\n+      }\n+      if (entry.message !== null) {\n+        causeEntry.message = entry.message\n+      }\n+      return causeEntry\n+    })\n   }\n \n   return snapshot\n",
			"results": [
				{
					"modelId": "anthropic/claude-opus-4.5",
					"tier": "frontier",
					"status": "success",
					"message": "fix(next-devtools): hide empty error messages in dev overlay",
					"latencyMs": 1920,
					"costUsd": 0.023185,
					"inputTokens": 4552,
					"outputTokens": 17,
					"humanReview": {
						"overallScore": 3,
						"notes": "Pending manual review.",
						"winnerFlag": false
					},
					"providerMetadata": {
						"anthropic": {
							"usage": {
								"input_tokens": 4552,
								"output_tokens": 17,
								"cache_creation_input_tokens": 0,
								"cache_read_input_tokens": 0,
								"cache_creation": {
									"ephemeral_5m_input_tokens": 0,
									"ephemeral_1h_input_tokens": 0
								},
								"service_tier": "standard",
								"inference_geo": "not_available"
							},
							"cacheCreationInputTokens": 0,
							"stopSequence": null,
							"iterations": null,
							"container": null,
							"contextManagement": null
						},
						"gateway": {
							"routing": {
								"originalModelId": "anthropic/claude-opus-4.5",
								"resolvedProvider": "anthropic",
								"resolvedProviderApiModelId": "claude-opus-4-5-20251101",
								"internalResolvedModelId": "anthropic:claude-opus-4-5-20251101",
								"fallbacksAvailable": ["vertexAnthropic", "bedrock"],
								"internalReasoning": "Selected anthropic as preferred provider for claude-opus-4.5. 2 fallback(s) available: vertexAnthropic, bedrock",
								"planningReasoning": "BYOK credentials available for: anthropic, vertexAnthropic. System credentials planned for: anthropic, vertexAnthropic, bedrock. Total execution order: anthropic(BYOK)  vertexAnthropic(BYOK)  anthropic(system)  vertexAnthropic(system)  bedrock(system)",
								"canonicalSlug": "anthropic/claude-opus-4.5",
								"finalProvider": "anthropic",
								"attempts": [
									{
										"provider": "anthropic",
										"internalModelId": "anthropic:claude-opus-4-5-20251101",
										"providerApiModelId": "claude-opus-4-5-20251101",
										"credentialType": "byok",
										"success": true,
										"startTime": 1416163.313623,
										"endTime": 1417745.090174,
										"statusCode": 200,
										"providerResponseId": "msg_016o49e3FuQp61poUTBmNpPZ"
									}
								],
								"modelAttemptCount": 1,
								"modelAttempts": [
									{
										"modelId": "anthropic/claude-opus-4.5",
										"canonicalSlug": "anthropic/claude-opus-4.5",
										"success": false,
										"providerAttemptCount": 0,
										"providerAttempts": []
									}
								],
								"totalProviderAttemptCount": 0
							},
							"cost": "0",
							"marketCost": "0.023185",
							"generationId": "gen_01KJ3S5368VTV0F35RC5MM2T8G",
							"billableWebSearchCalls": 0
						}
					}
				},
				{
					"modelId": "cerebras/llama3.1-8b",
					"tier": "small",
					"status": "success",
					"message": "feat(core): trim error messages",
					"latencyMs": 735,
					"costUsd": 0.0003628,
					"inputTokens": 3621,
					"outputTokens": 7,
					"humanReview": {
						"overallScore": 3,
						"notes": "Pending manual review.",
						"winnerFlag": false
					},
					"providerMetadata": {
						"cerebras": {
							"acceptedPredictionTokens": 0,
							"rejectedPredictionTokens": 0
						},
						"gateway": {
							"routing": {
								"originalModelId": "cerebras/llama3.1-8b",
								"resolvedProvider": "cerebras",
								"resolvedProviderApiModelId": "llama3.1-8b",
								"internalResolvedModelId": "cerebras:llama3.1-8b",
								"fallbacksAvailable": [],
								"internalReasoning": "Used provider hint 'cerebras' for llama3.1-8b. 0 fallback(s) available: ",
								"planningReasoning": "System credentials planned for: cerebras. Total execution order: cerebras(system)",
								"canonicalSlug": "meta/llama-3.1-8b",
								"finalProvider": "cerebras",
								"attempts": [
									{
										"provider": "cerebras",
										"internalModelId": "cerebras:llama3.1-8b",
										"providerApiModelId": "llama3.1-8b",
										"credentialType": "system",
										"success": true,
										"startTime": 2337258.159093,
										"endTime": 2337643.535881,
										"statusCode": 200
									}
								],
								"modelAttemptCount": 1,
								"modelAttempts": [
									{
										"modelId": "cerebras/llama3.1-8b",
										"canonicalSlug": "cerebras/llama3.1-8b",
										"success": false,
										"providerAttemptCount": 0,
										"providerAttempts": []
									}
								],
								"totalProviderAttemptCount": 0
							},
							"cost": "0.0003628",
							"marketCost": "0.0003628",
							"generationId": "gen_01KJ3S52F7G01FJCZRRTGE5RB1",
							"billableWebSearchCalls": 0
						}
					}
				},
				{
					"modelId": "google/gemini-3-pro-preview",
					"tier": "frontier",
					"status": "success",
					"message": "fix(",
					"latencyMs": 3421,
					"costUsd": 0.010166,
					"inputTokens": 4387,
					"outputTokens": 116,
					"humanReview": {
						"overallScore": 3,
						"notes": "Pending manual review.",
						"winnerFlag": false
					},
					"providerMetadata": {
						"vertex": {
							"promptFeedback": null,
							"groundingMetadata": null,
							"urlContextMetadata": null,
							"safetyRatings": null,
							"usageMetadata": {
								"thoughtsTokenCount": 114,
								"promptTokenCount": 4387,
								"candidatesTokenCount": 2,
								"totalTokenCount": 4503,
								"trafficType": "ON_DEMAND"
							}
						},
						"gateway": {
							"routing": {
								"originalModelId": "google/gemini-3-pro-preview",
								"resolvedProvider": "vertex",
								"resolvedProviderApiModelId": "gemini-3-pro-preview",
								"internalResolvedModelId": "vertex:gemini-3-pro-preview",
								"fallbacksAvailable": ["google"],
								"internalReasoning": "Selected vertex as preferred provider for gemini-3-pro-preview. 1 fallback(s) available: google",
								"planningReasoning": "BYOK credentials available for: vertex. System credentials planned for: vertex, google. Total execution order: vertex(BYOK)  vertex(system)  google(system)",
								"canonicalSlug": "google/gemini-3-pro-preview",
								"finalProvider": "vertex",
								"attempts": [
									{
										"provider": "vertex",
										"internalModelId": "vertex:gemini-3-pro-preview",
										"providerApiModelId": "gemini-3-pro-preview",
										"credentialType": "byok",
										"success": true,
										"startTime": 90363.736908,
										"endTime": 93438.57595,
										"statusCode": 200,
										"providerResponseId": "4Imbaa3dF_HHptQPpvnY2Qc"
									}
								],
								"modelAttemptCount": 1,
								"modelAttempts": [
									{
										"modelId": "google/gemini-3-pro-preview",
										"canonicalSlug": "google/gemini-3-pro-preview",
										"success": false,
										"providerAttemptCount": 0,
										"providerAttempts": []
									}
								],
								"totalProviderAttemptCount": 0
							},
							"cost": "0",
							"marketCost": "0.010166",
							"generationId": "gen_01KJ3S54YRWSJN6SN88MJR4V2Q",
							"billableWebSearchCalls": 0
						}
					}
				},
				{
					"modelId": "mistral/ministral-3b",
					"tier": "small",
					"status": "success",
					"message": "feat(devtools): add null checks for error messages in error display",
					"latencyMs": 498,
					"costUsd": 0.00015552,
					"inputTokens": 3874,
					"outputTokens": 14,
					"humanReview": {
						"overallScore": 3,
						"notes": "Pending manual review.",
						"winnerFlag": false
					},
					"providerMetadata": {
						"gateway": {
							"routing": {
								"originalModelId": "mistral/ministral-3b",
								"resolvedProvider": "mistral",
								"resolvedProviderApiModelId": "ministral-3b-latest",
								"internalResolvedModelId": "mistral:ministral-3b-latest",
								"fallbacksAvailable": [],
								"internalReasoning": "Selected mistral as preferred provider for ministral-3b. 0 fallback(s) available: ",
								"planningReasoning": "System credentials planned for: mistral. Total execution order: mistral(system)",
								"canonicalSlug": "mistral/ministral-3b",
								"finalProvider": "mistral",
								"attempts": [
									{
										"provider": "mistral",
										"internalModelId": "mistral:ministral-3b-latest",
										"providerApiModelId": "ministral-3b-latest",
										"credentialType": "system",
										"success": true,
										"startTime": 1644950.134219,
										"endTime": 1645249.775524,
										"statusCode": 200
									}
								],
								"modelAttemptCount": 1,
								"modelAttempts": [
									{
										"modelId": "mistral/ministral-3b",
										"canonicalSlug": "mistral/ministral-3b",
										"success": false,
										"providerAttemptCount": 0,
										"providerAttempts": []
									}
								],
								"totalProviderAttemptCount": 0
							},
							"cost": "0.00015552",
							"marketCost": "0.00015552",
							"generationId": "gen_01KJ3S52B14RZB7PXXT641KPFG",
							"billableWebSearchCalls": 0
						}
					}
				},
				{
					"modelId": "openai/gpt-5.2",
					"tier": "frontier",
					"status": "success",
					"message": "fix(dev-overlay): trim and omit empty error descriptions and causes",
					"latencyMs": 2048,
					"costUsd": 0.00656775,
					"inputTokens": 3625,
					"outputTokens": 16,
					"humanReview": {
						"overallScore": 3,
						"notes": "Pending manual review.",
						"winnerFlag": false
					},
					"providerMetadata": {
						"openai": {
							"responseId": "resp_0564b5277ad8554e00699b89de9b54819ead9e93518570c302",
							"serviceTier": "default"
						},
						"gateway": {
							"routing": {
								"originalModelId": "openai/gpt-5.2",
								"resolvedProvider": "openai",
								"resolvedProviderApiModelId": "gpt-5.2",
								"internalResolvedModelId": "openai:gpt-5.2",
								"fallbacksAvailable": ["azure"],
								"internalReasoning": "Selected openai as preferred provider for gpt-5.2. 1 fallback(s) available: azure",
								"planningReasoning": "BYOK credentials available for: openai. System credentials planned for: openai, azure. Total execution order: openai(BYOK)  openai(system)  azure(system)",
								"canonicalSlug": "openai/gpt-5.2",
								"finalProvider": "openai",
								"attempts": [
									{
										"provider": "openai",
										"internalModelId": "openai:gpt-5.2",
										"providerApiModelId": "gpt-5.2",
										"credentialType": "byok",
										"success": true,
										"startTime": 650299.313453,
										"endTime": 652092.607759,
										"statusCode": 200,
										"providerResponseId": "resp_0564b5277ad8554e00699b89de9b54819ead9e93518570c302"
									}
								],
								"modelAttemptCount": 1,
								"modelAttempts": [
									{
										"modelId": "openai/gpt-5.2",
										"canonicalSlug": "openai/gpt-5.2",
										"success": false,
										"providerAttemptCount": 0,
										"providerAttempts": []
									}
								],
								"totalProviderAttemptCount": 0
							},
							"cost": "0",
							"marketCost": "0.00656775",
							"generationId": "gen_01KJ3S52T154KBSXVJG51Y5NC5",
							"billableWebSearchCalls": 0
						}
					}
				},
				{
					"modelId": "xai/grok-code-fast-1",
					"tier": "small",
					"status": "success",
					"message": "fix(dev-overlay): trim and hide empty error messages",
					"latencyMs": 12916,
					"costUsd": 0.00053954,
					"inputTokens": 3721,
					"outputTokens": 309,
					"humanReview": {
						"overallScore": 3,
						"notes": "Pending manual review.",
						"winnerFlag": false
					},
					"providerMetadata": {
						"gateway": {
							"routing": {
								"originalModelId": "xai/grok-code-fast-1",
								"resolvedProvider": "xai",
								"resolvedProviderApiModelId": "grok-code-fast-1",
								"internalResolvedModelId": "xai:grok-code-fast-1",
								"fallbacksAvailable": [],
								"internalReasoning": "Selected xai as preferred provider for grok-code-fast-1. 0 fallback(s) available: ",
								"planningReasoning": "System credentials planned for: xai. Total execution order: xai(system)",
								"canonicalSlug": "xai/grok-code-fast-1",
								"finalProvider": "xai",
								"attempts": [
									{
										"provider": "xai",
										"internalModelId": "xai:grok-code-fast-1",
										"providerApiModelId": "grok-code-fast-1",
										"credentialType": "system",
										"success": true,
										"startTime": 990699.508482,
										"endTime": 1003278.284606,
										"statusCode": 200
									}
								],
								"modelAttemptCount": 1,
								"modelAttempts": [
									{
										"modelId": "xai/grok-code-fast-1",
										"canonicalSlug": "xai/grok-code-fast-1",
										"success": false,
										"providerAttemptCount": 0,
										"providerAttempts": []
									}
								],
								"totalProviderAttemptCount": 0
							},
							"cost": "0.00053954",
							"marketCost": "0.00053954",
							"generationId": "gen_01KJ3S52F7KECJN03X82HYZ87F",
							"billableWebSearchCalls": 0
						}
					}
				}
			]
		},
		{
			"id": "zed-9304e02b-agent-allow-the-agent-to-reprompt-a",
			"title": "agent: Allow the agent to reprompt an existing subagent (#49737)",
			"sourceRepo": "zed-industries/zed",
			"sourceCommitUrl": "https://github.com/zed-industries/zed/commit/9304e02b9a2e72169d9082f56fce2c632a02a8eb",
			"diff": "diff --git a/crates/agent/src/agent.rs b/crates/agent/src/agent.rs\nindex 8cea712c19cebb..4e8691c0b5017e 100644\n--- a/crates/agent/src/agent.rs\n+++ b/crates/agent/src/agent.rs\n@@ -1611,11 +1611,60 @@ impl NativeThreadEnvironment {\n             agent.register_session(subagent_thread.clone(), cx)\n         })?;\n \n+        Self::prompt_subagent(\n+            session_id,\n+            subagent_thread,\n+            acp_thread,\n+            parent_thread_entity,\n+            initial_prompt,\n+            timeout,\n+            cx,\n+        )\n+    }\n+\n+    pub(crate) fn resume_subagent_thread(\n+        agent: WeakEntity<NativeAgent>,\n+        parent_thread_entity: Entity<Thread>,\n+        session_id: acp::SessionId,\n+        follow_up_prompt: String,\n+        timeout: Option<Duration>,\n+        cx: &mut App,\n+    ) -> Result<Rc<dyn SubagentHandle>> {\n+        let (subagent_thread, acp_thread) = agent.update(cx, |agent, _cx| {\n+            let session = agent\n+                .sessions\n+                .get(&session_id)\n+                .ok_or_else(|| anyhow!(\"No subagent session found with id {session_id}\"))?;\n+            anyhow::Ok((session.thread.clone(), session.acp_thread.clone()))\n+        })??;\n+\n+        Self::prompt_subagent(\n+            session_id,\n+            subagent_thread,\n+            acp_thread,\n+            parent_thread_entity,\n+            follow_up_prompt,\n+            timeout,\n+            cx,\n+        )\n+    }\n+\n+    fn prompt_subagent(\n+        session_id: acp::SessionId,\n+        subagent_thread: Entity<Thread>,\n+        acp_thread: Entity<acp_thread::AcpThread>,\n+        parent_thread_entity: Entity<Thread>,\n+        prompt: String,\n+        timeout: Option<Duration>,\n+        cx: &mut App,\n+    ) -> Result<Rc<dyn SubagentHandle>> {\n         parent_thread_entity.update(cx, |parent_thread, _cx| {\n             parent_thread.register_running_subagent(subagent_thread.downgrade())\n         });\n \n-        let task = acp_thread.update(cx, |agent, cx| agent.send(vec![initial_prompt.into()], cx));\n+        let task = acp_thread.update(cx, |acp_thread, cx| {\n+            acp_thread.send(vec![prompt.into()], cx)\n+        });\n \n         let timeout_timer = timeout.map(|d| cx.background_executor().timer(d));\n         let wait_for_prompt_to_complete = cx\n@@ -1708,6 +1757,24 @@ impl ThreadEnvironment for NativeThreadEnvironment {\n             cx,\n         )\n     }\n+\n+    fn resume_subagent(\n+        &self,\n+        parent_thread_entity: Entity<Thread>,\n+        session_id: acp::SessionId,\n+        follow_up_prompt: String,\n+        timeout: Option<Duration>,\n+        cx: &mut App,\n+    ) -> Result<Rc<dyn SubagentHandle>> {\n+        Self::resume_subagent_thread(\n+            self.agent.clone(),\n+            parent_thread_entity,\n+            session_id,\n+            follow_up_prompt,\n+            timeout,\n+            cx,\n+        )\n+    }\n }\n \n #[derive(Debug, Clone, Copy)]\n@@ -1737,21 +1804,20 @@ impl SubagentHandle for NativeSubagentHandle {\n         let parent_thread = self.parent_thread.clone();\n \n         cx.spawn(async move |cx| {\n-            match wait_for_prompt.await {\n-                SubagentInitialPromptResult::Completed => {}\n+            let result = match wait_for_prompt.await {\n+                SubagentInitialPromptResult::Completed => thread.read_with(cx, |thread, _cx| {\n+                    thread\n+                        .last_message()\n+                        .map(|m| m.to_markdown())\n+                        .context(\"No response from subagent\")\n+                }),\n                 SubagentInitialPromptResult::Timeout => {\n-                    return Err(anyhow!(\"The time to complete the task was exceeded.\"));\n+                    thread.update(cx, |thread, cx| thread.cancel(cx)).await;\n+                    Err(anyhow!(\"The time to complete the task was exceeded.\"))\n                 }\n-                SubagentInitialPromptResult::Cancelled => return Err(anyhow!(\"User cancelled\")),\n+                SubagentInitialPromptResult::Cancelled => Err(anyhow!(\"User cancelled\")),\n             };\n \n-            let result = thread.read_with(cx, |thread, _cx| {\n-                thread\n-                    .last_message()\n-                    .map(|m| m.to_markdown())\n-                    .context(\"No response from subagent\")\n-            });\n-\n             parent_thread\n                 .update(cx, |parent_thread, cx| {\n                     parent_thread.unregister_running_subagent(&subagent_session_id, cx)\ndiff --git a/crates/agent/src/tests/mod.rs b/crates/agent/src/tests/mod.rs\nindex d895557fc1dec3..2673e33d0e7c8a 100644\n--- a/crates/agent/src/tests/mod.rs\n+++ b/crates/agent/src/tests/mod.rs\n@@ -4233,6 +4233,7 @@ async fn test_subagent_tool_call_end_to_end(cx: &mut TestAppContext) {\n     let subagent_tool_input = SubagentToolInput {\n         label: \"label\".to_string(),\n         task: \"subagent task prompt\".to_string(),\n+        session_id: None,\n         timeout_secs: None,\n     };\n     let subagent_tool_use = LanguageModelToolUse {\n@@ -4383,6 +4384,7 @@ async fn test_subagent_tool_call_cancellation_during_task_prompt(cx: &mut TestAp\n     let subagent_tool_input = SubagentToolInput {\n         label: \"label\".to_string(),\n         task: \"subagent task prompt\".to_string(),\n+        session_id: None,\n         timeout_secs: None,\n     };\n     let subagent_tool_use = LanguageModelToolUse {\n@@ -4460,6 +4462,195 @@ async fn test_subagent_tool_call_cancellation_during_task_prompt(cx: &mut TestAp\n     });\n }\n \n+#[gpui::test]\n+async fn test_subagent_tool_resume_session(cx: &mut TestAppContext) {\n+    init_test(cx);\n+    cx.update(|cx| {\n+        LanguageModelRegistry::test(cx);\n+    });\n+    cx.update(|cx| {\n+        cx.update_flags(true, vec![\"subagents\".to_string()]);\n+    });\n+\n+    let fs = FakeFs::new(cx.executor());\n+    fs.insert_tree(\n+        \"/\",\n+        json!({\n+            \"a\": {\n+                \"b.md\": \"Lorem\"\n+            }\n+        }),\n+    )\n+    .await;\n+    let project = Project::test(fs.clone(), [path!(\"/a\").as_ref()], cx).await;\n+    let thread_store = cx.new(|cx| ThreadStore::new(cx));\n+    let agent = NativeAgent::new(\n+        project.clone(),\n+        thread_store.clone(),\n+        Templates::new(),\n+        None,\n+        fs.clone(),\n+        &mut cx.to_async(),\n+    )\n+    .await\n+    .unwrap();\n+    let connection = Rc::new(NativeAgentConnection(agent.clone()));\n+\n+    let acp_thread = cx\n+        .update(|cx| {\n+            connection\n+                .clone()\n+                .new_session(project.clone(), Path::new(\"\"), cx)\n+        })\n+        .await\n+        .unwrap();\n+    let session_id = acp_thread.read_with(cx, |thread, _| thread.session_id().clone());\n+    let thread = agent.read_with(cx, |agent, _| {\n+        agent.sessions.get(&session_id).unwrap().thread.clone()\n+    });\n+    let model = Arc::new(FakeLanguageModel::default());\n+\n+    thread.update(cx, |thread, cx| {\n+        thread.set_model(model.clone(), cx);\n+    });\n+    cx.run_until_parked();\n+\n+    // === First turn: create subagent ===\n+    let send = acp_thread.update(cx, |thread, cx| thread.send_raw(\"First prompt\", cx));\n+    cx.run_until_parked();\n+    model.send_last_completion_stream_text_chunk(\"spawning subagent\");\n+    let subagent_tool_input = SubagentToolInput {\n+        label: \"initial task\".to_string(),\n+        task: \"do the first task\".to_string(),\n+        session_id: None,\n+        timeout_secs: None,\n+    };\n+    let subagent_tool_use = LanguageModelToolUse {\n+        id: \"subagent_1\".into(),\n+        name: SubagentTool::NAME.into(),\n+        raw_input: serde_json::to_string(&subagent_tool_input).unwrap(),\n+        input: serde_json::to_value(&subagent_tool_input).unwrap(),\n+        is_input_complete: true,\n+        thought_signature: None,\n+    };\n+    model.send_last_completion_stream_event(LanguageModelCompletionEvent::ToolUse(\n+        subagent_tool_use,\n+    ));\n+    model.end_last_completion_stream();\n+\n+    cx.run_until_parked();\n+\n+    let subagent_session_id = thread.read_with(cx, |thread, cx| {\n+        thread\n+            .running_subagent_ids(cx)\n+            .get(0)\n+            .expect(\"subagent thread should be running\")\n+            .clone()\n+    });\n+\n+    let subagent_acp_thread = agent.read_with(cx, |agent, _cx| {\n+        agent\n+            .sessions\n+            .get(&subagent_session_id)\n+            .expect(\"subagent session should exist\")\n+            .acp_thread\n+            .clone()\n+    });\n+\n+    // Subagent responds\n+    model.send_last_completion_stream_text_chunk(\"first task response\");\n+    model.end_last_completion_stream();\n+\n+    cx.run_until_parked();\n+\n+    // Parent model responds to complete first turn\n+    model.send_last_completion_stream_text_chunk(\"First response\");\n+    model.end_last_completion_stream();\n+\n+    send.await.unwrap();\n+\n+    // Verify subagent is no longer running\n+    thread.read_with(cx, |thread, cx| {\n+        assert!(\n+            thread.running_subagent_ids(cx).is_empty(),\n+            \"subagent should not be running after completion\"\n+        );\n+    });\n+\n+    // === Second turn: resume subagent with session_id ===\n+    let send2 = acp_thread.update(cx, |thread, cx| thread.send_raw(\"Follow up\", cx));\n+    cx.run_until_parked();\n+    model.send_last_completion_stream_text_chunk(\"resuming subagent\");\n+    let resume_tool_input = SubagentToolInput {\n+        label: \"follow-up task\".to_string(),\n+        task: \"do the follow-up task\".to_string(),\n+        session_id: Some(subagent_session_id.clone()),\n+        timeout_secs: None,\n+    };\n+    let resume_tool_use = LanguageModelToolUse {\n+        id: \"subagent_2\".into(),\n+        name: SubagentTool::NAME.into(),\n+        raw_input: serde_json::to_string(&resume_tool_input).unwrap(),\n+        input: serde_json::to_value(&resume_tool_input).unwrap(),\n+        is_input_complete: true,\n+        thought_signature: None,\n+    };\n+    model.send_last_completion_stream_event(LanguageModelCompletionEvent::ToolUse(resume_tool_use));\n+    model.end_last_completion_stream();\n+\n+    cx.run_until_parked();\n+\n+    // Subagent should be running again with the same session\n+    thread.read_with(cx, |thread, cx| {\n+        let running = thread.running_subagent_ids(cx);\n+        assert_eq!(running.len(), 1, \"subagent should be running\");\n+        assert_eq!(running[0], subagent_session_id, \"should be same session\");\n+    });\n+\n+    // Subagent responds to follow-up\n+    model.send_last_completion_stream_text_chunk(\"follow-up task response\");\n+    model.end_last_completion_stream();\n+\n+    cx.run_until_parked();\n+\n+    // Parent model responds to complete second turn\n+    model.send_last_completion_stream_text_chunk(\"Second response\");\n+    model.end_last_completion_stream();\n+\n+    send2.await.unwrap();\n+\n+    // Verify subagent is no longer running\n+    thread.read_with(cx, |thread, cx| {\n+        assert!(\n+            thread.running_subagent_ids(cx).is_empty(),\n+            \"subagent should not be running after resume completion\"\n+        );\n+    });\n+\n+    // Verify the subagent's acp thread has both conversation turns\n+    assert_eq!(\n+        subagent_acp_thread.read_with(cx, |thread, cx| thread.to_markdown(cx)),\n+        indoc! {\"\n+            ## User\n+\n+            do the first task\n+\n+            ## Assistant\n+\n+            first task response\n+\n+            ## User\n+\n+            do the follow-up task\n+\n+            ## Assistant\n+\n+            follow-up task response\n+\n+        \"}\n+    );\n+}\n+\n #[gpui::test]\n async fn test_subagent_tool_is_present_when_feature_flag_enabled(cx: &mut TestAppContext) {\n     init_test(cx);\ndiff --git a/crates/agent/src/thread.rs b/crates/agent/src/thread.rs\nindex 93024a97073e6f..cc8a46987d0d01 100644\n--- a/crates/agent/src/thread.rs\n+++ b/crates/agent/src/thread.rs\n@@ -621,6 +621,19 @@ pub trait ThreadEnvironment {\n         timeout: Option<Duration>,\n         cx: &mut App,\n     ) -> Result<Rc<dyn SubagentHandle>>;\n+\n+    fn resume_subagent(\n+        &self,\n+        _parent_thread: Entity<Thread>,\n+        _session_id: acp::SessionId,\n+        _follow_up_prompt: String,\n+        _timeout: Option<Duration>,\n+        _cx: &mut App,\n+    ) -> Result<Rc<dyn SubagentHandle>> {\n+        Err(anyhow::anyhow!(\n+            \"Resuming subagent sessions is not supported\"\n+        ))\n+    }\n }\n \n #[derive(Debug)]\ndiff --git a/crates/agent/src/tools/subagent_tool.rs b/crates/agent/src/tools/subagent_tool.rs\nindex 1c9487dcbe611f..8212192d7bfc51 100644\n--- a/crates/agent/src/tools/subagent_tool.rs\n+++ b/crates/agent/src/tools/subagent_tool.rs\n@@ -23,6 +23,8 @@ use crate::{AgentTool, Thread, ThreadEnvironment, ToolCallEventStream};\n ///\n /// You will receive only the agent's final message as output.\n ///\n+/// If a response (success or error) includes a session_id, you can send a follow-up message to that session by passing the session_id back. This is useful for multi-turn conversations with a subagent, asking clarifying questions about its output, or retrying after timeouts or transient failures.\n+///\n /// Note:\n /// - Agents cannot use tools you don't have access to.\n /// - If spawning multiple agents that might write to the filesystem, provide guidance on how to avoid conflicts (e.g. assign each to different directories).\n@@ -32,6 +34,9 @@ pub struct SubagentToolInput {\n     pub label: String,\n     /// Describe the task for the agent to perform. Be specific about what you want accomplished. Include all necessary context (file paths, requirements, constraints) since the agent cannot see your conversation.\n     pub task: String,\n+    /// Optional session ID of an existing subagent to continue a conversation with. When provided, the task is sent as a follow-up message to that session instead of creating a new one. Use this to ask clarifying questions, request changes based on previous output, or retry after errors.\n+    #[serde(default)]\n+    pub session_id: Option<acp::SessionId>,\n     /// Optional maximum runtime in seconds. The purpose of this timeout is to prevent the agent from getting stuck in infinite loops, NOT to estimate task duration. Be generous if setting. If not set, the agent runs until it completes or is cancelled.\n     #[serde(default)]\n     pub timeout_secs: Option<u64>,\n@@ -45,18 +50,18 @@ pub enum SubagentToolOutput {\n         output: String,\n     },\n     Error {\n+        #[serde(skip_serializing_if = \"Option::is_none\")]\n+        #[serde(default)]\n+        session_id: Option<acp::SessionId>,\n         error: String,\n     },\n }\n \n impl From<SubagentToolOutput> for LanguageModelToolResultContent {\n     fn from(output: SubagentToolOutput) -> Self {\n-        match output {\n-            output @ SubagentToolOutput::Success { .. } => serde_json::to_string(&output)\n-                .unwrap_or_else(|e| format!(\"Failed to serialize subagent output: {e}\"))\n-                .into(),\n-            SubagentToolOutput::Error { error } => error.into(),\n-        }\n+        serde_json::to_string(&output)\n+            .unwrap_or_else(|e| format!(\"Failed to serialize subagent output: {e}\"))\n+            .into()\n     }\n }\n \n@@ -103,25 +108,37 @@ impl AgentTool for SubagentTool {\n     ) -> Task<Result<Self::Output, Self::Output>> {\n         let Some(parent_thread_entity) = self.parent_thread.upgrade() else {\n             return Task::ready(Err(SubagentToolOutput::Error {\n+                session_id: None,\n                 error: \"Parent thread no longer exists\".to_string(),\n             }));\n         };\n \n-        let subagent = match self.environment.create_subagent(\n-            parent_thread_entity,\n-            input.label,\n-            input.task,\n-            input.timeout_secs.map(|secs| Duration::from_secs(secs)),\n-            cx,\n-        ) {\n+        let subagent = if let Some(session_id) = input.session_id {\n+            self.environment.resume_subagent(\n+                parent_thread_entity,\n+                session_id,\n+                input.task,\n+                input.timeout_secs.map(Duration::from_secs),\n+                cx,\n+            )\n+        } else {\n+            self.environment.create_subagent(\n+                parent_thread_entity,\n+                input.label,\n+                input.task,\n+                input.timeout_secs.map(Duration::from_secs),\n+                cx,\n+            )\n+        };\n+        let subagent = match subagent {\n             Ok(subagent) => subagent,\n             Err(err) => {\n                 return Task::ready(Err(SubagentToolOutput::Error {\n+                    session_id: None,\n                     error: err.to_string(),\n                 }));\n             }\n         };\n-\n         let subagent_session_id = subagent.id();\n \n         event_stream.subagent_spawned(subagent_session_id.clone());\n@@ -137,6 +154,7 @@ impl AgentTool for SubagentTool {\n                     .wait_for_output(cx)\n                     .await\n                     .map_err(|e| SubagentToolOutput::Error {\n+                        session_id: Some(subagent_session_id.clone()),\n                         error: e.to_string(),\n                     })?;\n             Ok(SubagentToolOutput::Success {\n@@ -153,17 +171,20 @@ impl AgentTool for SubagentTool {\n         event_stream: ToolCallEventStream,\n         _cx: &mut App,\n     ) -> Result<()> {\n-        match output {\n-            SubagentToolOutput::Success { session_id, .. } => {\n-                event_stream.subagent_spawned(session_id.clone());\n-                let meta = acp::Meta::from_iter([(\n-                    SUBAGENT_SESSION_ID_META_KEY.into(),\n-                    session_id.to_string().into(),\n-                )]);\n-                event_stream.update_fields_with_meta(acp::ToolCallUpdateFields::new(), Some(meta));\n-            }\n-            SubagentToolOutput::Error { .. } => {}\n+        let session_id = match &output {\n+            SubagentToolOutput::Success { session_id, .. } => Some(session_id),\n+            SubagentToolOutput::Error { session_id, .. } => session_id.as_ref(),\n+        };\n+\n+        if let Some(session_id) = session_id {\n+            event_stream.subagent_spawned(session_id.clone());\n+            let meta = acp::Meta::from_iter([(\n+                SUBAGENT_SESSION_ID_META_KEY.into(),\n+                session_id.to_string().into(),\n+            )]);\n+            event_stream.update_fields_with_meta(acp::ToolCallUpdateFields::new(), Some(meta));\n         }\n+\n         Ok(())\n     }\n }\n",
			"results": [
				{
					"modelId": "anthropic/claude-opus-4.5",
					"tier": "frontier",
					"status": "success",
					"message": "feat(agent): add subagent session resumption support",
					"latencyMs": 1929,
					"costUsd": 0.03168,
					"inputTokens": 6261,
					"outputTokens": 15,
					"humanReview": {
						"overallScore": 3,
						"notes": "Pending manual review.",
						"winnerFlag": false
					},
					"providerMetadata": {
						"anthropic": {
							"usage": {
								"input_tokens": 6261,
								"output_tokens": 15,
								"cache_creation_input_tokens": 0,
								"cache_read_input_tokens": 0,
								"cache_creation": {
									"ephemeral_5m_input_tokens": 0,
									"ephemeral_1h_input_tokens": 0
								},
								"service_tier": "standard",
								"inference_geo": "not_available"
							},
							"cacheCreationInputTokens": 0,
							"stopSequence": null,
							"iterations": null,
							"container": null,
							"contextManagement": null
						},
						"gateway": {
							"routing": {
								"originalModelId": "anthropic/claude-opus-4.5",
								"resolvedProvider": "anthropic",
								"resolvedProviderApiModelId": "claude-opus-4-5-20251101",
								"internalResolvedModelId": "anthropic:claude-opus-4-5-20251101",
								"fallbacksAvailable": ["vertexAnthropic", "bedrock"],
								"internalReasoning": "Selected anthropic as preferred provider for claude-opus-4.5. 2 fallback(s) available: vertexAnthropic, bedrock",
								"planningReasoning": "BYOK credentials available for: anthropic, vertexAnthropic. System credentials planned for: anthropic, vertexAnthropic, bedrock. Total execution order: anthropic(BYOK)  vertexAnthropic(BYOK)  anthropic(system)  vertexAnthropic(system)  bedrock(system)",
								"canonicalSlug": "anthropic/claude-opus-4.5",
								"finalProvider": "anthropic",
								"attempts": [
									{
										"provider": "anthropic",
										"internalModelId": "anthropic:claude-opus-4-5-20251101",
										"providerApiModelId": "claude-opus-4-5-20251101",
										"credentialType": "byok",
										"success": true,
										"startTime": 2351521.670286,
										"endTime": 2353118.323611,
										"statusCode": 200,
										"providerResponseId": "msg_01QLKeRonp42ToXopjPoeuSP"
									}
								],
								"modelAttemptCount": 1,
								"modelAttempts": [
									{
										"modelId": "anthropic/claude-opus-4.5",
										"canonicalSlug": "anthropic/claude-opus-4.5",
										"success": false,
										"providerAttemptCount": 0,
										"providerAttempts": []
									}
								],
								"totalProviderAttemptCount": 0
							},
							"cost": "0",
							"marketCost": "0.03168",
							"generationId": "gen_01KJ3S5G1MH05FY3MBW7M1M4KX",
							"billableWebSearchCalls": 0
						}
					}
				},
				{
					"modelId": "cerebras/llama3.1-8b",
					"tier": "small",
					"status": "success",
					"message": "feat(core): add subagent resume functionality",
					"latencyMs": 993,
					"costUsd": 0.0004559,
					"inputTokens": 4678,
					"outputTokens": 9,
					"humanReview": {
						"overallScore": 3,
						"notes": "Pending manual review.",
						"winnerFlag": false
					},
					"providerMetadata": {
						"cerebras": {
							"acceptedPredictionTokens": 0,
							"rejectedPredictionTokens": 0
						},
						"gateway": {
							"routing": {
								"originalModelId": "cerebras/llama3.1-8b",
								"resolvedProvider": "cerebras",
								"resolvedProviderApiModelId": "llama3.1-8b",
								"internalResolvedModelId": "cerebras:llama3.1-8b",
								"fallbacksAvailable": [],
								"internalReasoning": "Used provider hint 'cerebras' for llama3.1-8b. 0 fallback(s) available: ",
								"planningReasoning": "System credentials planned for: cerebras. Total execution order: cerebras(system)",
								"canonicalSlug": "meta/llama-3.1-8b",
								"finalProvider": "cerebras",
								"attempts": [
									{
										"provider": "cerebras",
										"internalModelId": "cerebras:llama3.1-8b",
										"providerApiModelId": "llama3.1-8b",
										"credentialType": "system",
										"success": true,
										"startTime": 2350042.74263,
										"endTime": 2350698.483239,
										"statusCode": 200
									}
								],
								"modelAttemptCount": 1,
								"modelAttempts": [
									{
										"modelId": "cerebras/llama3.1-8b",
										"canonicalSlug": "cerebras/llama3.1-8b",
										"success": false,
										"providerAttemptCount": 0,
										"providerAttempts": []
									}
								],
								"totalProviderAttemptCount": 0
							},
							"cost": "0.0004559",
							"marketCost": "0.0004559",
							"generationId": "gen_01KJ3S5F2JQXATP0B5AYAZBQV6",
							"billableWebSearchCalls": 0
						}
					}
				},
				{
					"modelId": "google/gemini-3-pro-preview",
					"tier": "frontier",
					"status": "success",
					"message": "feat(agent): allow",
					"latencyMs": 4450,
					"costUsd": 0.013182,
					"inputTokens": 5895,
					"outputTokens": 116,
					"humanReview": {
						"overallScore": 3,
						"notes": "Pending manual review.",
						"winnerFlag": false
					},
					"providerMetadata": {
						"vertex": {
							"promptFeedback": null,
							"groundingMetadata": null,
							"urlContextMetadata": null,
							"safetyRatings": null,
							"usageMetadata": {
								"thoughtsTokenCount": 111,
								"promptTokenCount": 5895,
								"candidatesTokenCount": 5,
								"totalTokenCount": 6011,
								"trafficType": "ON_DEMAND"
							}
						},
						"gateway": {
							"routing": {
								"originalModelId": "google/gemini-3-pro-preview",
								"resolvedProvider": "vertex",
								"resolvedProviderApiModelId": "gemini-3-pro-preview",
								"internalResolvedModelId": "vertex:gemini-3-pro-preview",
								"fallbacksAvailable": ["google"],
								"internalReasoning": "Selected vertex as preferred provider for gemini-3-pro-preview. 1 fallback(s) available: google",
								"planningReasoning": "BYOK credentials available for: vertex. System credentials planned for: vertex, google. Total execution order: vertex(BYOK)  vertex(system)  google(system)",
								"canonicalSlug": "google/gemini-3-pro-preview",
								"finalProvider": "vertex",
								"attempts": [
									{
										"provider": "vertex",
										"internalModelId": "vertex:gemini-3-pro-preview",
										"providerApiModelId": "gemini-3-pro-preview",
										"credentialType": "byok",
										"success": true,
										"startTime": 2353249.780579,
										"endTime": 2357367.470929,
										"statusCode": 200,
										"providerResponseId": "7Ymbaff2GdbVle0PwMGKmAo"
									}
								],
								"modelAttemptCount": 1,
								"modelAttempts": [
									{
										"modelId": "google/gemini-3-pro-preview",
										"canonicalSlug": "google/gemini-3-pro-preview",
										"success": false,
										"providerAttemptCount": 0,
										"providerAttempts": []
									}
								],
								"totalProviderAttemptCount": 0
							},
							"cost": "0",
							"marketCost": "0.013182",
							"generationId": "gen_01KJ3S5HQMFQKZFBVWB9DV4AXH",
							"billableWebSearchCalls": 0
						}
					}
				},
				{
					"modelId": "mistral/ministral-3b",
					"tier": "small",
					"status": "success",
					"message": "feat(agent): add subagent session resume functionality",
					"latencyMs": 735,
					"costUsd": 0.00020016,
					"inputTokens": 4993,
					"outputTokens": 11,
					"humanReview": {
						"overallScore": 3,
						"notes": "Pending manual review.",
						"winnerFlag": false
					},
					"providerMetadata": {
						"gateway": {
							"routing": {
								"originalModelId": "mistral/ministral-3b",
								"resolvedProvider": "mistral",
								"resolvedProviderApiModelId": "ministral-3b-latest",
								"internalResolvedModelId": "mistral:ministral-3b-latest",
								"fallbacksAvailable": [],
								"internalReasoning": "Selected mistral as preferred provider for ministral-3b. 0 fallback(s) available: ",
								"planningReasoning": "System credentials planned for: mistral. Total execution order: mistral(system)",
								"canonicalSlug": "mistral/ministral-3b",
								"finalProvider": "mistral",
								"attempts": [
									{
										"provider": "mistral",
										"internalModelId": "mistral:ministral-3b-latest",
										"providerApiModelId": "ministral-3b-latest",
										"credentialType": "system",
										"success": true,
										"startTime": 2350549.258774,
										"endTime": 2350933.335153,
										"statusCode": 200
									}
								],
								"modelAttemptCount": 1,
								"modelAttempts": [
									{
										"modelId": "mistral/ministral-3b",
										"canonicalSlug": "mistral/ministral-3b",
										"success": false,
										"providerAttemptCount": 0,
										"providerAttempts": []
									}
								],
								"totalProviderAttemptCount": 0
							},
							"cost": "0.00020016",
							"marketCost": "0.00020016",
							"generationId": "gen_01KJ3S5F2NX9CHPNX1K6QNXHH8",
							"billableWebSearchCalls": 0
						}
					}
				},
				{
					"modelId": "openai/gpt-5.2",
					"tier": "frontier",
					"status": "success",
					"message": "feat(agent): add subagent session resume support with session_id and tests",
					"latencyMs": 1984,
					"costUsd": 0.00842625,
					"inputTokens": 4671,
					"outputTokens": 18,
					"humanReview": {
						"overallScore": 3,
						"notes": "Pending manual review.",
						"winnerFlag": false
					},
					"providerMetadata": {
						"openai": {
							"responseId": "resp_0b5d455366b7172700699b89ebd6f081a086c4cdba46947907",
							"serviceTier": "default"
						},
						"gateway": {
							"routing": {
								"originalModelId": "openai/gpt-5.2",
								"resolvedProvider": "openai",
								"resolvedProviderApiModelId": "gpt-5.2",
								"internalResolvedModelId": "openai:gpt-5.2",
								"fallbacksAvailable": ["azure"],
								"internalReasoning": "Selected openai as preferred provider for gpt-5.2. 1 fallback(s) available: azure",
								"planningReasoning": "BYOK credentials available for: openai. System credentials planned for: openai, azure. Total execution order: openai(BYOK)  openai(system)  azure(system)",
								"canonicalSlug": "openai/gpt-5.2",
								"finalProvider": "openai",
								"attempts": [
									{
										"provider": "openai",
										"internalModelId": "openai:gpt-5.2",
										"providerApiModelId": "gpt-5.2",
										"credentialType": "byok",
										"success": true,
										"startTime": 2350171.989442,
										"endTime": 2351804.485108,
										"statusCode": 200,
										"providerResponseId": "resp_0b5d455366b7172700699b89ebd6f081a086c4cdba46947907"
									}
								],
								"modelAttemptCount": 1,
								"modelAttempts": [
									{
										"modelId": "openai/gpt-5.2",
										"canonicalSlug": "openai/gpt-5.2",
										"success": false,
										"providerAttemptCount": 0,
										"providerAttempts": []
									}
								],
								"totalProviderAttemptCount": 0
							},
							"cost": "0",
							"marketCost": "0.00842625",
							"generationId": "gen_01KJ3S5FSRA1BQH6EAT6JWMKNN",
							"billableWebSearchCalls": 0
						}
					}
				},
				{
					"modelId": "xai/grok-code-fast-1",
					"tier": "small",
					"status": "success",
					"message": "feat(agent): add support for resuming subagent sessions",
					"latencyMs": 16004,
					"costUsd": 0.00148808,
					"inputTokens": 4831,
					"outputTokens": 394,
					"humanReview": {
						"overallScore": 3,
						"notes": "Pending manual review.",
						"winnerFlag": false
					},
					"providerMetadata": {
						"gateway": {
							"routing": {
								"originalModelId": "xai/grok-code-fast-1",
								"resolvedProvider": "xai",
								"resolvedProviderApiModelId": "grok-code-fast-1",
								"internalResolvedModelId": "xai:grok-code-fast-1",
								"fallbacksAvailable": [],
								"internalReasoning": "Selected xai as preferred provider for grok-code-fast-1. 0 fallback(s) available: ",
								"planningReasoning": "System credentials planned for: xai. Total execution order: xai(system)",
								"canonicalSlug": "xai/grok-code-fast-1",
								"finalProvider": "xai",
								"attempts": [
									{
										"provider": "xai",
										"internalModelId": "xai:grok-code-fast-1",
										"providerApiModelId": "grok-code-fast-1",
										"credentialType": "system",
										"success": true,
										"startTime": 2349710.967422,
										"endTime": 2365349.705567,
										"statusCode": 200
									}
								],
								"modelAttemptCount": 1,
								"modelAttempts": [
									{
										"modelId": "xai/grok-code-fast-1",
										"canonicalSlug": "xai/grok-code-fast-1",
										"success": false,
										"providerAttemptCount": 0,
										"providerAttempts": []
									}
								],
								"totalProviderAttemptCount": 0
							},
							"cost": "0.00148808",
							"marketCost": "0.00148808",
							"generationId": "gen_01KJ3S5F3B0543YBCDSAPQWGS9",
							"billableWebSearchCalls": 0
						}
					}
				}
			]
		}
	],
	"aggregate": {
		"byModel": [
			{
				"modelId": "mistral/ministral-3b",
				"tier": "small",
				"successCount": 3,
				"errorCount": 0,
				"avgLatencyMs": 847.67,
				"avgCostUsd": 0.0003831,
				"avgScore": 3
			},
			{
				"modelId": "xai/grok-code-fast-1",
				"tier": "small",
				"successCount": 3,
				"errorCount": 0,
				"avgLatencyMs": 17235.67,
				"avgCostUsd": 0.0022539,
				"avgScore": 3
			},
			{
				"modelId": "cerebras/llama3.1-8b",
				"tier": "small",
				"successCount": 2,
				"errorCount": 1,
				"avgLatencyMs": 864,
				"avgCostUsd": 0.0004094,
				"avgScore": 3
			},
			{
				"modelId": "openai/gpt-5.2",
				"tier": "frontier",
				"successCount": 3,
				"errorCount": 0,
				"avgLatencyMs": 2193.33,
				"avgCostUsd": 0.015911,
				"avgScore": 3
			},
			{
				"modelId": "anthropic/claude-opus-4.5",
				"tier": "frontier",
				"successCount": 3,
				"errorCount": 0,
				"avgLatencyMs": 2145.67,
				"avgCostUsd": 0.059025,
				"avgScore": 3
			},
			{
				"modelId": "google/gemini-3-pro-preview",
				"tier": "frontier",
				"successCount": 3,
				"errorCount": 0,
				"avgLatencyMs": 5741,
				"avgCostUsd": 0.023198,
				"avgScore": 3
			}
		],
		"smallTopModelId": "mistral/ministral-3b",
		"frontierTopModelId": "anthropic/claude-opus-4.5"
	}
}
